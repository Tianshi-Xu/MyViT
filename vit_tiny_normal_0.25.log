2024-04-06 20:31:19,469 - train - INFO - Training with a single process on 1 GPUs.
2024-04-06 20:31:28,360 - train - INFO - Model vit_9_12_64 created, param count:2766144
2024-04-06 20:31:28,374 - train - INFO - Using native Torch AMP. Training in mixed precision.
2024-04-06 20:31:28,375 - train - INFO - Scheduled epochs: 160
2024-04-06 20:31:28,649 - train - INFO - Verifying teacher model
2024-04-06 20:31:30,169 - train - INFO - Test: [   0/78]  Time: 1.519 (1.519)  Loss:  0.9009 (0.9009)  Acc@1: 82.0312 (82.0312)  Acc@5: 94.5312 (94.5312)
2024-04-06 20:31:32,334 - train - INFO - Test: [  50/78]  Time: 0.042 (0.072)  Loss:  1.7285 (1.6075)  Acc@1: 58.5938 (63.1127)  Acc@5: 83.5938 (84.9112)
2024-04-06 20:31:33,517 - train - INFO - Test: [  78/78]  Time: 0.037 (0.062)  Loss:  1.8408 (1.6375)  Acc@1: 56.2500 (62.6500)  Acc@5: 75.0000 (84.3200)
2024-04-06 20:31:33,517 - train - INFO - Verifying initial model
2024-04-06 20:31:33,710 - train - INFO - Test: [   0/78]  Time: 0.191 (0.191)  Loss:  5.3594 (5.3594)  Acc@1:  0.0000 ( 0.0000)  Acc@5:  0.0000 ( 0.0000)
2024-04-06 20:31:38,686 - train - INFO - Test: [  50/78]  Time: 0.091 (0.101)  Loss:  5.3398 (5.2082)  Acc@1:  0.0000 ( 1.2102)  Acc@5:  0.0000 ( 5.0398)
2024-04-06 20:31:41,167 - train - INFO - Test: [  78/78]  Time: 0.052 (0.097)  Loss:  5.0898 (5.2248)  Acc@1:  0.0000 ( 0.9800)  Acc@5:  0.0000 ( 4.1400)
2024-04-06 20:31:42,790 - train - INFO - Train: 0 [   0/781 (  0%)]  Loss:  5.336673 (5.3367)  Time: 1.621s,   78.97/s  (1.621s,   78.97/s)  LR: 1.000e-06  Data: 0.338 (0.338)
2024-04-06 20:32:26,541 - train - INFO - Train: 0 [  50/781 (  6%)]  Loss:  5.300611 (5.3230)  Time: 0.882s,  145.08/s  (0.890s,  143.89/s)  LR: 1.000e-06  Data: 0.004 (0.012)
2024-04-06 20:33:07,848 - train - INFO - Train: 0 [ 100/781 ( 13%)]  Loss:  5.325736 (5.3197)  Time: 0.893s,  143.34/s  (0.858s,  149.16/s)  LR: 1.000e-06  Data: 0.011 (0.009)
2024-04-06 20:33:47,456 - train - INFO - Train: 0 [ 150/781 ( 19%)]  Loss:  5.316051 (5.3188)  Time: 0.812s,  157.60/s  (0.836s,  153.05/s)  LR: 1.000e-06  Data: 0.009 (0.008)
2024-04-06 20:34:26,288 - train - INFO - Train: 0 [ 200/781 ( 26%)]  Loss:  5.344341 (5.3193)  Time: 0.778s,  164.62/s  (0.821s,  155.82/s)  LR: 1.000e-06  Data: 0.005 (0.008)
2024-04-06 20:35:05,004 - train - INFO - Train: 0 [ 250/781 ( 32%)]  Loss:  5.323534 (5.3174)  Time: 0.820s,  156.19/s  (0.812s,  157.62/s)  LR: 1.000e-06  Data: 0.009 (0.007)
2024-04-06 20:35:49,841 - train - INFO - Train: 0 [ 300/781 ( 38%)]  Loss:  5.304840 (5.3165)  Time: 0.895s,  142.97/s  (0.826s,  154.94/s)  LR: 1.000e-06  Data: 0.006 (0.007)
2024-04-06 20:36:35,366 - train - INFO - Train: 0 [ 350/781 ( 45%)]  Loss:  5.295053 (5.3160)  Time: 0.935s,  136.86/s  (0.838s,  152.72/s)  LR: 1.000e-06  Data: 0.006 (0.007)
2024-04-06 20:37:20,402 - train - INFO - Train: 0 [ 400/781 ( 51%)]  Loss:  5.306694 (5.3153)  Time: 0.904s,  141.62/s  (0.846s,  151.31/s)  LR: 1.000e-06  Data: 0.006 (0.007)
2024-04-06 20:38:05,458 - train - INFO - Train: 0 [ 450/781 ( 58%)]  Loss:  5.319454 (5.3146)  Time: 0.889s,  144.03/s  (0.852s,  150.23/s)  LR: 1.000e-06  Data: 0.008 (0.007)
2024-04-06 20:38:50,451 - train - INFO - Train: 0 [ 500/781 ( 64%)]  Loss:  5.293317 (5.3137)  Time: 0.914s,  140.06/s  (0.857s,  149.39/s)  LR: 1.000e-06  Data: 0.005 (0.007)
2024-04-06 20:39:36,004 - train - INFO - Train: 0 [ 550/781 ( 71%)]  Loss:  5.317171 (5.3128)  Time: 0.858s,  149.23/s  (0.862s,  148.54/s)  LR: 1.000e-06  Data: 0.006 (0.007)
2024-04-06 20:40:22,623 - train - INFO - Train: 0 [ 600/781 ( 77%)]  Loss:  5.299723 (5.3120)  Time: 0.925s,  138.32/s  (0.868s,  147.53/s)  LR: 1.000e-06  Data: 0.004 (0.007)
2024-04-06 20:41:07,863 - train - INFO - Train: 0 [ 650/781 ( 83%)]  Loss:  5.306880 (5.3109)  Time: 0.877s,  145.88/s  (0.870s,  147.05/s)  LR: 1.000e-06  Data: 0.005 (0.007)
2024-04-06 20:41:53,607 - train - INFO - Train: 0 [ 700/781 ( 90%)]  Loss:  5.291200 (5.3102)  Time: 0.927s,  138.06/s  (0.874s,  146.51/s)  LR: 1.000e-06  Data: 0.007 (0.007)
2024-04-06 20:42:40,142 - train - INFO - Train: 0 [ 750/781 ( 96%)]  Loss:  5.278983 (5.3094)  Time: 0.898s,  142.54/s  (0.877s,  145.88/s)  LR: 1.000e-06  Data: 0.005 (0.007)
2024-04-06 20:43:07,370 - train - INFO - Train: 0 [ 780/781 (100%)]  Loss:  5.301353 (5.3089)  Time: 0.877s,  145.94/s  (0.879s,  145.69/s)  LR: 1.000e-06  Data: 0.000 (0.007)
2024-04-06 20:43:07,370 - train - INFO - True
2024-04-06 20:43:07,382 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,405 - train - INFO - True
2024-04-06 20:43:07,409 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,440 - train - INFO - True
2024-04-06 20:43:07,441 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,468 - train - INFO - True
2024-04-06 20:43:07,469 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,494 - train - INFO - True
2024-04-06 20:43:07,494 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,517 - train - INFO - True
2024-04-06 20:43:07,518 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,538 - train - INFO - True
2024-04-06 20:43:07,540 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,559 - train - INFO - True
2024-04-06 20:43:07,564 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,582 - train - INFO - True
2024-04-06 20:43:07,585 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,603 - train - INFO - True
2024-04-06 20:43:07,606 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,623 - train - INFO - True
2024-04-06 20:43:07,624 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,640 - train - INFO - True
2024-04-06 20:43:07,641 - train - INFO - alphas:tensor([0.2001, 0.2001, 0.2000, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,657 - train - INFO - True
2024-04-06 20:43:07,658 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,675 - train - INFO - True
2024-04-06 20:43:07,675 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,692 - train - INFO - True
2024-04-06 20:43:07,693 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,709 - train - INFO - True
2024-04-06 20:43:07,711 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,727 - train - INFO - True
2024-04-06 20:43:07,731 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,747 - train - INFO - True
2024-04-06 20:43:07,752 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.2000, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,769 - train - INFO - True
2024-04-06 20:43:07,774 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,790 - train - INFO - True
2024-04-06 20:43:07,791 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,807 - train - INFO - True
2024-04-06 20:43:07,808 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,824 - train - INFO - True
2024-04-06 20:43:07,825 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,842 - train - INFO - True
2024-04-06 20:43:07,842 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,859 - train - INFO - True
2024-04-06 20:43:07,860 - train - INFO - alphas:tensor([0.2002, 0.2000, 0.1999, 0.2000, 0.2000], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,876 - train - INFO - True
2024-04-06 20:43:07,881 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,898 - train - INFO - True
2024-04-06 20:43:07,902 - train - INFO - alphas:tensor([0.2002, 0.2000, 0.2000, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,919 - train - INFO - True
2024-04-06 20:43:07,923 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,939 - train - INFO - True
2024-04-06 20:43:07,944 - train - INFO - alphas:tensor([0.2001, 0.1999, 0.1999, 0.2000, 0.2000], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,961 - train - INFO - True
2024-04-06 20:43:07,961 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,978 - train - INFO - True
2024-04-06 20:43:07,978 - train - INFO - alphas:tensor([0.2002, 0.2001, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:07,995 - train - INFO - True
2024-04-06 20:43:07,996 - train - INFO - alphas:tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:08,004 - train - INFO - True
2024-04-06 20:43:08,005 - train - INFO - alphas:tensor([0.2000, 0.2001, 0.2000, 0.2000, 0.2000], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:08,015 - train - INFO - True
2024-04-06 20:43:08,016 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:08,033 - train - INFO - True
2024-04-06 20:43:08,033 - train - INFO - alphas:tensor([0.2002, 0.2002, 0.1999, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:08,050 - train - INFO - True
2024-04-06 20:43:08,053 - train - INFO - alphas:tensor([0.2000, 0.2000, 0.2001, 0.1999, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:08,061 - train - INFO - True
2024-04-06 20:43:08,063 - train - INFO - alphas:tensor([0.1999, 0.1999, 0.2000, 0.2001, 0.2001], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:08,072 - train - INFO - True
2024-04-06 20:43:08,074 - train - INFO - alphas:tensor([0.2502, 0.2500, 0.2499, 0.2499], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:43:08,090 - train - INFO - avg block size:1.5135135135135136
2024-04-06 20:43:08,091 - train - INFO - current latency ratio:tensor(0.9056)
2024-04-06 20:43:08,410 - train - INFO - Test: [   0/78]  Time: 0.316 (0.316)  Loss:  5.1680 (5.1680)  Acc@1:  0.7812 ( 0.7812)  Acc@5:  7.8125 ( 7.8125)
2024-04-06 20:43:15,551 - train - INFO - Test: [  50/78]  Time: 0.102 (0.146)  Loss:  5.3164 (5.1839)  Acc@1:  0.0000 ( 1.3940)  Acc@5:  1.5625 ( 6.5104)
2024-04-06 20:43:19,355 - train - INFO - Test: [  78/78]  Time: 0.071 (0.143)  Loss:  4.9844 (5.1843)  Acc@1:  0.0000 ( 1.4000)  Acc@5:  0.0000 ( 6.0300)
2024-04-06 20:43:20,705 - train - INFO - Train: 1 [   0/781 (  0%)]  Loss:  5.305011 (5.3050)  Time: 1.287s,   99.46/s  (1.287s,   99.46/s)  LR: 5.090e-05  Data: 0.198 (0.198)
2024-04-06 20:44:06,465 - train - INFO - Train: 1 [  50/781 (  6%)]  Loss:  5.242572 (5.2721)  Time: 0.863s,  148.37/s  (0.922s,  138.76/s)  LR: 5.090e-05  Data: 0.006 (0.010)
2024-04-06 20:44:52,334 - train - INFO - Train: 1 [ 100/781 ( 13%)]  Loss:  5.192552 (5.2514)  Time: 0.891s,  143.69/s  (0.920s,  139.14/s)  LR: 5.090e-05  Data: 0.006 (0.008)
2024-04-06 20:45:37,684 - train - INFO - Train: 1 [ 150/781 ( 19%)]  Loss:  5.127788 (5.2304)  Time: 0.924s,  138.47/s  (0.916s,  139.79/s)  LR: 5.090e-05  Data: 0.006 (0.007)
2024-04-06 20:46:24,102 - train - INFO - Train: 1 [ 200/781 ( 26%)]  Loss:  5.187922 (5.2095)  Time: 0.995s,  128.66/s  (0.919s,  139.31/s)  LR: 5.090e-05  Data: 0.006 (0.007)
2024-04-06 20:47:12,271 - train - INFO - Train: 1 [ 250/781 ( 32%)]  Loss:  5.101547 (5.1892)  Time: 0.865s,  148.02/s  (0.928s,  137.98/s)  LR: 5.090e-05  Data: 0.005 (0.007)
2024-04-06 20:47:57,639 - train - INFO - Train: 1 [ 300/781 ( 38%)]  Loss:  5.037466 (5.1699)  Time: 0.944s,  135.56/s  (0.924s,  138.48/s)  LR: 5.090e-05  Data: 0.007 (0.007)
2024-04-06 20:48:42,662 - train - INFO - Train: 1 [ 350/781 ( 45%)]  Loss:  5.044949 (5.1527)  Time: 0.909s,  140.74/s  (0.921s,  138.99/s)  LR: 5.090e-05  Data: 0.005 (0.007)
2024-04-06 20:49:30,428 - train - INFO - Train: 1 [ 400/781 ( 51%)]  Loss:  5.122958 (5.1371)  Time: 0.844s,  151.73/s  (0.925s,  138.35/s)  LR: 5.090e-05  Data: 0.005 (0.007)
2024-04-06 20:50:19,288 - train - INFO - Train: 1 [ 450/781 ( 58%)]  Loss:  4.978192 (5.1226)  Time: 0.923s,  138.63/s  (0.931s,  137.49/s)  LR: 5.090e-05  Data: 0.004 (0.007)
2024-04-06 20:51:08,801 - train - INFO - Train: 1 [ 500/781 ( 64%)]  Loss:  4.928103 (5.1081)  Time: 1.079s,  118.62/s  (0.937s,  136.63/s)  LR: 5.090e-05  Data: 0.009 (0.007)
2024-04-06 20:51:58,649 - train - INFO - Train: 1 [ 550/781 ( 71%)]  Loss:  4.939293 (5.0961)  Time: 0.968s,  132.24/s  (0.942s,  135.84/s)  LR: 5.090e-05  Data: 0.010 (0.007)
2024-04-06 20:52:51,427 - train - INFO - Train: 1 [ 600/781 ( 77%)]  Loss:  4.756433 (5.0850)  Time: 0.885s,  144.60/s  (0.952s,  134.49/s)  LR: 5.090e-05  Data: 0.004 (0.008)
2024-04-06 20:53:44,092 - train - INFO - Train: 1 [ 650/781 ( 83%)]  Loss:  5.147636 (5.0741)  Time: 1.016s,  125.97/s  (0.960s,  133.40/s)  LR: 5.090e-05  Data: 0.031 (0.008)
2024-04-06 20:54:36,381 - train - INFO - Train: 1 [ 700/781 ( 90%)]  Loss:  5.087844 (5.0659)  Time: 1.064s,  120.32/s  (0.966s,  132.55/s)  LR: 5.090e-05  Data: 0.005 (0.008)
2024-04-06 20:55:24,038 - train - INFO - Train: 1 [ 750/781 ( 96%)]  Loss:  5.022375 (5.0565)  Time: 0.906s,  141.22/s  (0.965s,  132.66/s)  LR: 5.090e-05  Data: 0.005 (0.008)
2024-04-06 20:55:51,371 - train - INFO - Train: 1 [ 780/781 (100%)]  Loss:  5.011888 (5.0500)  Time: 0.890s,  143.81/s  (0.963s,  132.95/s)  LR: 5.090e-05  Data: 0.000 (0.008)
2024-04-06 20:55:51,371 - train - INFO - True
2024-04-06 20:55:51,377 - train - INFO - alphas:tensor([0.2120, 0.2120, 0.1922, 0.1920, 0.1919], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,402 - train - INFO - True
2024-04-06 20:55:51,403 - train - INFO - alphas:tensor([0.2099, 0.2097, 0.1940, 0.1932, 0.1932], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,426 - train - INFO - True
2024-04-06 20:55:51,427 - train - INFO - alphas:tensor([0.2153, 0.2140, 0.1909, 0.1899, 0.1898], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,448 - train - INFO - True
2024-04-06 20:55:51,449 - train - INFO - alphas:tensor([0.2138, 0.2123, 0.1925, 0.1909, 0.1905], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,469 - train - INFO - True
2024-04-06 20:55:51,471 - train - INFO - alphas:tensor([0.2143, 0.2142, 0.1907, 0.1905, 0.1903], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,489 - train - INFO - True
2024-04-06 20:55:51,492 - train - INFO - alphas:tensor([0.2141, 0.2122, 0.1920, 0.1909, 0.1908], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,509 - train - INFO - True
2024-04-06 20:55:51,513 - train - INFO - alphas:tensor([0.2136, 0.2133, 0.1910, 0.1910, 0.1910], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,530 - train - INFO - True
2024-04-06 20:55:51,534 - train - INFO - alphas:tensor([0.2136, 0.2106, 0.1925, 0.1917, 0.1915], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,551 - train - INFO - True
2024-04-06 20:55:51,555 - train - INFO - alphas:tensor([0.2137, 0.2132, 0.1907, 0.1909, 0.1914], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,571 - train - INFO - True
2024-04-06 20:55:51,572 - train - INFO - alphas:tensor([0.2121, 0.2089, 0.1937, 0.1929, 0.1924], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,589 - train - INFO - True
2024-04-06 20:55:51,590 - train - INFO - alphas:tensor([0.2138, 0.2114, 0.1923, 0.1908, 0.1916], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,607 - train - INFO - True
2024-04-06 20:55:51,607 - train - INFO - alphas:tensor([0.2118, 0.2059, 0.1952, 0.1936, 0.1935], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,624 - train - INFO - True
2024-04-06 20:55:51,625 - train - INFO - alphas:tensor([0.2119, 0.2124, 0.1923, 0.1918, 0.1916], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,635 - train - INFO - True
2024-04-06 20:55:51,636 - train - INFO - alphas:tensor([0.2113, 0.2085, 0.1941, 0.1931, 0.1930], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,653 - train - INFO - True
2024-04-06 20:55:51,658 - train - INFO - alphas:tensor([0.2132, 0.2104, 0.1938, 0.1915, 0.1911], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,675 - train - INFO - True
2024-04-06 20:55:51,680 - train - INFO - alphas:tensor([0.2092, 0.2067, 0.1948, 0.1945, 0.1947], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,696 - train - INFO - True
2024-04-06 20:55:51,701 - train - INFO - alphas:tensor([0.2116, 0.2116, 0.1925, 0.1923, 0.1920], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,718 - train - INFO - True
2024-04-06 20:55:51,723 - train - INFO - alphas:tensor([0.2124, 0.2090, 0.1923, 0.1933, 0.1930], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,739 - train - INFO - True
2024-04-06 20:55:51,740 - train - INFO - alphas:tensor([0.2104, 0.2077, 0.1950, 0.1936, 0.1934], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,757 - train - INFO - True
2024-04-06 20:55:51,757 - train - INFO - alphas:tensor([0.2080, 0.2042, 0.1970, 0.1953, 0.1954], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,774 - train - INFO - True
2024-04-06 20:55:51,775 - train - INFO - alphas:tensor([0.2117, 0.2107, 0.1932, 0.1923, 0.1921], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,792 - train - INFO - True
2024-04-06 20:55:51,792 - train - INFO - alphas:tensor([0.2113, 0.2078, 0.1938, 0.1934, 0.1938], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,809 - train - INFO - True
2024-04-06 20:55:51,810 - train - INFO - alphas:tensor([0.2098, 0.2073, 0.1945, 0.1942, 0.1942], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,827 - train - INFO - True
2024-04-06 20:55:51,831 - train - INFO - alphas:tensor([0.2040, 0.2018, 0.1981, 0.1982, 0.1979], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,848 - train - INFO - True
2024-04-06 20:55:51,852 - train - INFO - alphas:tensor([0.2131, 0.2123, 0.1921, 0.1913, 0.1913], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,868 - train - INFO - True
2024-04-06 20:55:51,873 - train - INFO - alphas:tensor([0.2131, 0.2082, 0.1935, 0.1923, 0.1929], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,889 - train - INFO - True
2024-04-06 20:55:51,894 - train - INFO - alphas:tensor([0.2094, 0.2061, 0.1962, 0.1948, 0.1935], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,911 - train - INFO - True
2024-04-06 20:55:51,911 - train - INFO - alphas:tensor([0.2062, 0.2028, 0.1973, 0.1967, 0.1971], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,928 - train - INFO - True
2024-04-06 20:55:51,929 - train - INFO - alphas:tensor([0.2123, 0.2112, 0.1923, 0.1920, 0.1921], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,946 - train - INFO - True
2024-04-06 20:55:51,946 - train - INFO - alphas:tensor([0.2114, 0.2085, 0.1935, 0.1935, 0.1932], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,963 - train - INFO - True
2024-04-06 20:55:51,964 - train - INFO - alphas:tensor([0.2084, 0.2068, 0.1965, 0.1947, 0.1936], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,980 - train - INFO - True
2024-04-06 20:55:51,981 - train - INFO - alphas:tensor([0.2103, 0.2054, 0.1955, 0.1939, 0.1948], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:51,998 - train - INFO - True
2024-04-06 20:55:52,001 - train - INFO - alphas:tensor([0.2099, 0.2100, 0.1932, 0.1934, 0.1934], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:52,011 - train - INFO - True
2024-04-06 20:55:52,016 - train - INFO - alphas:tensor([0.2097, 0.2080, 0.1939, 0.1943, 0.1942], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:52,033 - train - INFO - True
2024-04-06 20:55:52,037 - train - INFO - alphas:tensor([0.2112, 0.2084, 0.1956, 0.1925, 0.1923], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:52,054 - train - INFO - True
2024-04-06 20:55:52,058 - train - INFO - alphas:tensor([0.2088, 0.2058, 0.1958, 0.1948, 0.1948], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:52,075 - train - INFO - True
2024-04-06 20:55:52,076 - train - INFO - alphas:tensor([0.2697, 0.2491, 0.2403, 0.2409], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:55:52,093 - train - INFO - avg block size:1.054054054054054
2024-04-06 20:55:52,093 - train - INFO - current latency ratio:tensor(0.9653)
2024-04-06 20:55:52,434 - train - INFO - Test: [   0/78]  Time: 0.338 (0.338)  Loss:  3.3086 (3.3086)  Acc@1: 46.0938 (46.0938)  Acc@5: 75.0000 (75.0000)
2024-04-06 20:55:59,735 - train - INFO - Test: [  50/78]  Time: 0.086 (0.150)  Loss:  4.0430 (4.1105)  Acc@1: 25.0000 (18.9185)  Acc@5: 46.0938 (41.3756)
2024-04-06 20:56:03,771 - train - INFO - Test: [  78/78]  Time: 0.132 (0.148)  Loss:  4.0859 (4.0948)  Acc@1: 12.5000 (18.7300)  Acc@5: 43.7500 (41.6300)
2024-04-06 20:56:04,905 - train - INFO - Train: 2 [   0/781 (  0%)]  Loss:  4.991206 (4.9912)  Time: 1.073s,  119.31/s  (1.073s,  119.31/s)  LR: 1.008e-04  Data: 0.183 (0.183)
2024-04-06 20:57:02,398 - train - INFO - Train: 2 [  50/781 (  6%)]  Loss:  5.030072 (4.9221)  Time: 1.042s,  122.88/s  (1.148s,  111.47/s)  LR: 1.008e-04  Data: 0.008 (0.010)
2024-04-06 20:58:01,076 - train - INFO - Train: 2 [ 100/781 ( 13%)]  Loss:  4.988201 (4.9126)  Time: 1.598s,   80.12/s  (1.161s,  110.27/s)  LR: 1.008e-04  Data: 0.005 (0.008)
2024-04-06 20:58:42,761 - train - INFO - Train: 2 [ 150/781 ( 19%)]  Loss:  4.860048 (4.8912)  Time: 0.774s,  165.43/s  (1.052s,  121.62/s)  LR: 1.008e-04  Data: 0.005 (0.007)
2024-04-06 20:59:23,432 - train - INFO - Train: 2 [ 200/781 ( 26%)]  Loss:  4.960278 (4.8778)  Time: 0.843s,  151.81/s  (0.993s,  128.90/s)  LR: 1.008e-04  Data: 0.007 (0.007)
2024-04-06 21:00:09,698 - train - INFO - Train: 2 [ 250/781 ( 32%)]  Loss:  4.976381 (4.8712)  Time: 0.850s,  150.66/s  (0.980s,  130.68/s)  LR: 1.008e-04  Data: 0.008 (0.007)
2024-04-06 21:00:54,703 - train - INFO - Train: 2 [ 300/781 ( 38%)]  Loss:  4.676555 (4.8650)  Time: 0.918s,  139.51/s  (0.966s,  132.46/s)  LR: 1.008e-04  Data: 0.004 (0.007)
2024-04-06 21:01:47,865 - train - INFO - Train: 2 [ 350/781 ( 45%)]  Loss:  4.525564 (4.8532)  Time: 1.033s,  123.94/s  (0.980s,  130.60/s)  LR: 1.008e-04  Data: 0.017 (0.008)
2024-04-06 21:02:38,370 - train - INFO - Train: 2 [ 400/781 ( 51%)]  Loss:  4.772824 (4.8481)  Time: 1.143s,  111.97/s  (0.984s,  130.10/s)  LR: 1.008e-04  Data: 0.007 (0.009)
2024-04-06 21:03:26,573 - train - INFO - Train: 2 [ 450/781 ( 58%)]  Loss:  4.518845 (4.8404)  Time: 0.846s,  151.31/s  (0.982s,  130.39/s)  LR: 1.008e-04  Data: 0.008 (0.009)
2024-04-06 21:04:12,972 - train - INFO - Train: 2 [ 500/781 ( 64%)]  Loss:  4.529738 (4.8327)  Time: 0.864s,  148.11/s  (0.976s,  131.11/s)  LR: 1.008e-04  Data: 0.009 (0.009)
2024-04-06 21:05:01,246 - train - INFO - Train: 2 [ 550/781 ( 71%)]  Loss:  4.906682 (4.8282)  Time: 0.935s,  136.91/s  (0.975s,  131.24/s)  LR: 1.008e-04  Data: 0.007 (0.009)
2024-04-06 21:05:53,288 - train - INFO - Train: 2 [ 600/781 ( 77%)]  Loss:  4.818946 (4.8213)  Time: 1.066s,  120.04/s  (0.981s,  130.51/s)  LR: 1.008e-04  Data: 0.005 (0.009)
2024-04-06 21:06:44,085 - train - INFO - Train: 2 [ 650/781 ( 83%)]  Loss:  4.720860 (4.8142)  Time: 0.938s,  136.51/s  (0.983s,  130.15/s)  LR: 1.008e-04  Data: 0.006 (0.008)
2024-04-06 21:07:36,198 - train - INFO - Train: 2 [ 700/781 ( 90%)]  Loss:  4.503013 (4.8059)  Time: 0.911s,  140.47/s  (0.988s,  129.60/s)  LR: 1.008e-04  Data: 0.007 (0.008)
2024-04-06 21:08:28,917 - train - INFO - Train: 2 [ 750/781 ( 96%)]  Loss:  4.823325 (4.8005)  Time: 0.969s,  132.05/s  (0.992s,  129.02/s)  LR: 1.008e-04  Data: 0.008 (0.008)
2024-04-06 21:09:00,234 - train - INFO - Train: 2 [ 780/781 (100%)]  Loss:  4.311245 (4.7964)  Time: 1.150s,  111.32/s  (0.994s,  128.76/s)  LR: 1.008e-04  Data: 0.000 (0.008)
2024-04-06 21:09:00,235 - train - INFO - True
2024-04-06 21:09:00,238 - train - INFO - alphas:tensor([0.2212, 0.2191, 0.1860, 0.1868, 0.1869], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,279 - train - INFO - True
2024-04-06 21:09:00,281 - train - INFO - alphas:tensor([0.2208, 0.2159, 0.1873, 0.1873, 0.1886], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,314 - train - INFO - True
2024-04-06 21:09:00,316 - train - INFO - alphas:tensor([0.2424, 0.2358, 0.1754, 0.1732, 0.1732], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,345 - train - INFO - True
2024-04-06 21:09:00,347 - train - INFO - alphas:tensor([0.2419, 0.2284, 0.1773, 0.1766, 0.1758], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,381 - train - INFO - True
2024-04-06 21:09:00,384 - train - INFO - alphas:tensor([0.2336, 0.2294, 0.1787, 0.1791, 0.1793], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,423 - train - INFO - True
2024-04-06 21:09:00,424 - train - INFO - alphas:tensor([0.2341, 0.2228, 0.1814, 0.1808, 0.1809], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,456 - train - INFO - True
2024-04-06 21:09:00,457 - train - INFO - alphas:tensor([0.2356, 0.2319, 0.1776, 0.1774, 0.1776], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,486 - train - INFO - True
2024-04-06 21:09:00,487 - train - INFO - alphas:tensor([0.2339, 0.2244, 0.1832, 0.1795, 0.1791], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,515 - train - INFO - True
2024-04-06 21:09:00,516 - train - INFO - alphas:tensor([0.2357, 0.2304, 0.1772, 0.1777, 0.1790], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,545 - train - INFO - True
2024-04-06 21:09:00,548 - train - INFO - alphas:tensor([0.2332, 0.2210, 0.1830, 0.1814, 0.1815], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,577 - train - INFO - True
2024-04-06 21:09:00,578 - train - INFO - alphas:tensor([0.2364, 0.2263, 0.1795, 0.1779, 0.1799], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,606 - train - INFO - True
2024-04-06 21:09:00,611 - train - INFO - alphas:tensor([0.2311, 0.2137, 0.1864, 0.1852, 0.1837], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,639 - train - INFO - True
2024-04-06 21:09:00,640 - train - INFO - alphas:tensor([0.2305, 0.2257, 0.1808, 0.1814, 0.1816], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,669 - train - INFO - True
2024-04-06 21:09:00,670 - train - INFO - alphas:tensor([0.2329, 0.2148, 0.1849, 0.1834, 0.1840], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,698 - train - INFO - True
2024-04-06 21:09:00,699 - train - INFO - alphas:tensor([0.2351, 0.2233, 0.1837, 0.1792, 0.1787], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,727 - train - INFO - True
2024-04-06 21:09:00,728 - train - INFO - alphas:tensor([0.2260, 0.2150, 0.1877, 0.1849, 0.1863], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,756 - train - INFO - True
2024-04-06 21:09:00,757 - train - INFO - alphas:tensor([0.2273, 0.2226, 0.1829, 0.1835, 0.1837], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,785 - train - INFO - True
2024-04-06 21:09:00,787 - train - INFO - alphas:tensor([0.2324, 0.2161, 0.1832, 0.1839, 0.1843], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,815 - train - INFO - True
2024-04-06 21:09:00,820 - train - INFO - alphas:tensor([0.2326, 0.2222, 0.1838, 0.1810, 0.1804], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,848 - train - INFO - True
2024-04-06 21:09:00,853 - train - INFO - alphas:tensor([0.2253, 0.2126, 0.1894, 0.1860, 0.1867], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,881 - train - INFO - True
2024-04-06 21:09:00,882 - train - INFO - alphas:tensor([0.2296, 0.2229, 0.1825, 0.1824, 0.1826], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,910 - train - INFO - True
2024-04-06 21:09:00,911 - train - INFO - alphas:tensor([0.2356, 0.2162, 0.1824, 0.1827, 0.1831], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,939 - train - INFO - True
2024-04-06 21:09:00,940 - train - INFO - alphas:tensor([0.2329, 0.2199, 0.1829, 0.1821, 0.1822], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:00,969 - train - INFO - True
2024-04-06 21:09:00,970 - train - INFO - alphas:tensor([0.2175, 0.2074, 0.1923, 0.1912, 0.1916], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,001 - train - INFO - True
2024-04-06 21:09:01,004 - train - INFO - alphas:tensor([0.2301, 0.2236, 0.1813, 0.1823, 0.1827], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,032 - train - INFO - True
2024-04-06 21:09:01,035 - train - INFO - alphas:tensor([0.2348, 0.2151, 0.1846, 0.1819, 0.1836], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,064 - train - INFO - True
2024-04-06 21:09:01,067 - train - INFO - alphas:tensor([0.2372, 0.2225, 0.1830, 0.1797, 0.1775], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,103 - train - INFO - True
2024-04-06 21:09:01,104 - train - INFO - alphas:tensor([0.2238, 0.2112, 0.1899, 0.1874, 0.1877], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,132 - train - INFO - True
2024-04-06 21:09:01,133 - train - INFO - alphas:tensor([0.2293, 0.2242, 0.1817, 0.1823, 0.1825], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,161 - train - INFO - True
2024-04-06 21:09:01,162 - train - INFO - alphas:tensor([0.2326, 0.2185, 0.1833, 0.1826, 0.1830], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,188 - train - INFO - True
2024-04-06 21:09:01,191 - train - INFO - alphas:tensor([0.2403, 0.2284, 0.1797, 0.1762, 0.1753], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,214 - train - INFO - True
2024-04-06 21:09:01,219 - train - INFO - alphas:tensor([0.2287, 0.2154, 0.1877, 0.1837, 0.1844], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,240 - train - INFO - True
2024-04-06 21:09:01,242 - train - INFO - alphas:tensor([0.2240, 0.2220, 0.1841, 0.1847, 0.1852], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,263 - train - INFO - True
2024-04-06 21:09:01,265 - train - INFO - alphas:tensor([0.2235, 0.2156, 0.1866, 0.1871, 0.1872], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,285 - train - INFO - True
2024-04-06 21:09:01,286 - train - INFO - alphas:tensor([0.2398, 0.2310, 0.1771, 0.1762, 0.1759], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,305 - train - INFO - True
2024-04-06 21:09:01,306 - train - INFO - alphas:tensor([0.2271, 0.2182, 0.1855, 0.1844, 0.1848], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,324 - train - INFO - True
2024-04-06 21:09:01,324 - train - INFO - alphas:tensor([0.2983, 0.2427, 0.2283, 0.2308], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:09:01,342 - train - INFO - avg block size:1.0
2024-04-06 21:09:01,343 - train - INFO - current latency ratio:tensor(1.)
2024-04-06 21:09:01,704 - train - INFO - Test: [   0/78]  Time: 0.358 (0.358)  Loss:  2.5352 (2.5352)  Acc@1: 57.0312 (57.0312)  Acc@5: 78.1250 (78.1250)
2024-04-06 21:09:08,971 - train - INFO - Test: [  50/78]  Time: 0.185 (0.149)  Loss:  3.4727 (3.5510)  Acc@1: 34.3750 (27.9871)  Acc@5: 58.5938 (52.6195)
2024-04-06 21:09:12,960 - train - INFO - Test: [  78/78]  Time: 0.143 (0.147)  Loss:  3.7051 (3.5272)  Acc@1: 12.5000 (27.9000)  Acc@5: 43.7500 (53.5500)
2024-04-06 21:09:14,412 - train - INFO - Train: 3 [   0/781 (  0%)]  Loss:  4.764723 (4.7647)  Time: 1.384s,   92.51/s  (1.384s,   92.51/s)  LR: 1.507e-04  Data: 0.188 (0.188)
2024-04-06 21:10:06,136 - train - INFO - Train: 3 [  50/781 (  6%)]  Loss:  4.527878 (4.7055)  Time: 0.913s,  140.20/s  (1.041s,  122.92/s)  LR: 1.507e-04  Data: 0.008 (0.011)
2024-04-06 21:10:58,543 - train - INFO - Train: 3 [ 100/781 ( 13%)]  Loss:  4.430243 (4.7218)  Time: 1.233s,  103.81/s  (1.045s,  122.53/s)  LR: 1.507e-04  Data: 0.007 (0.009)
2024-04-06 21:11:50,658 - train - INFO - Train: 3 [ 150/781 ( 19%)]  Loss:  4.724802 (4.6845)  Time: 1.167s,  109.70/s  (1.044s,  122.63/s)  LR: 1.507e-04  Data: 0.019 (0.009)
2024-04-06 21:12:43,489 - train - INFO - Train: 3 [ 200/781 ( 26%)]  Loss:  4.338862 (4.6582)  Time: 1.163s,  110.08/s  (1.047s,  122.26/s)  LR: 1.507e-04  Data: 0.008 (0.009)
2024-04-06 21:13:34,999 - train - INFO - Train: 3 [ 250/781 ( 32%)]  Loss:  4.690128 (4.6538)  Time: 0.903s,  141.80/s  (1.044s,  122.65/s)  LR: 1.507e-04  Data: 0.007 (0.008)
2024-04-06 21:14:26,483 - train - INFO - Train: 3 [ 300/781 ( 38%)]  Loss:  4.293526 (4.6430)  Time: 1.198s,  106.81/s  (1.041s,  122.92/s)  LR: 1.507e-04  Data: 0.006 (0.008)
2024-04-06 21:15:18,656 - train - INFO - Train: 3 [ 350/781 ( 45%)]  Loss:  4.612906 (4.6347)  Time: 0.933s,  137.26/s  (1.042s,  122.89/s)  LR: 1.507e-04  Data: 0.008 (0.008)
2024-04-06 21:16:12,001 - train - INFO - Train: 3 [ 400/781 ( 51%)]  Loss:  4.623981 (4.6188)  Time: 0.995s,  128.65/s  (1.045s,  122.52/s)  LR: 1.507e-04  Data: 0.007 (0.008)
2024-04-06 21:17:02,785 - train - INFO - Train: 3 [ 450/781 ( 58%)]  Loss:  4.402076 (4.6121)  Time: 1.111s,  115.17/s  (1.042s,  122.90/s)  LR: 1.507e-04  Data: 0.008 (0.008)
2024-04-06 21:17:54,815 - train - INFO - Train: 3 [ 500/781 ( 64%)]  Loss:  4.571231 (4.5990)  Time: 1.085s,  118.01/s  (1.041s,  122.91/s)  LR: 1.507e-04  Data: 0.005 (0.008)
2024-04-06 21:18:46,271 - train - INFO - Train: 3 [ 550/781 ( 71%)]  Loss:  4.264570 (4.5915)  Time: 0.899s,  142.33/s  (1.040s,  123.04/s)  LR: 1.507e-04  Data: 0.007 (0.008)
2024-04-06 21:19:37,210 - train - INFO - Train: 3 [ 600/781 ( 77%)]  Loss:  4.755934 (4.5825)  Time: 0.915s,  139.91/s  (1.039s,  123.25/s)  LR: 1.507e-04  Data: 0.007 (0.008)
2024-04-06 21:20:29,965 - train - INFO - Train: 3 [ 650/781 ( 83%)]  Loss:  4.348668 (4.5738)  Time: 0.919s,  139.26/s  (1.040s,  123.10/s)  LR: 1.507e-04  Data: 0.008 (0.008)
2024-04-06 21:21:21,498 - train - INFO - Train: 3 [ 700/781 ( 90%)]  Loss:  4.770750 (4.5720)  Time: 1.113s,  115.01/s  (1.039s,  123.18/s)  LR: 1.507e-04  Data: 0.007 (0.008)
2024-04-06 21:22:14,853 - train - INFO - Train: 3 [ 750/781 ( 96%)]  Loss:  4.721567 (4.5642)  Time: 0.921s,  139.01/s  (1.041s,  122.96/s)  LR: 1.507e-04  Data: 0.008 (0.008)
2024-04-06 21:22:46,254 - train - INFO - Train: 3 [ 780/781 (100%)]  Loss:  4.659201 (4.5614)  Time: 1.141s,  112.22/s  (1.041s,  122.93/s)  LR: 1.507e-04  Data: 0.000 (0.008)
2024-04-06 21:22:46,255 - train - INFO - True
2024-04-06 21:22:46,261 - train - INFO - alphas:tensor([0.2292, 0.2244, 0.1807, 0.1826, 0.1831], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,292 - train - INFO - tau:0.99
2024-04-06 21:22:46,292 - train - INFO - True
2024-04-06 21:22:46,293 - train - INFO - alphas:tensor([0.2302, 0.2194, 0.1809, 0.1835, 0.1861], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,321 - train - INFO - tau:0.99
2024-04-06 21:22:46,321 - train - INFO - True
2024-04-06 21:22:46,322 - train - INFO - alphas:tensor([0.2805, 0.2571, 0.1563, 0.1530, 0.1531], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,346 - train - INFO - tau:0.99
2024-04-06 21:22:46,346 - train - INFO - True
2024-04-06 21:22:46,347 - train - INFO - alphas:tensor([0.2846, 0.2397, 0.1584, 0.1593, 0.1579], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,369 - train - INFO - tau:0.99
2024-04-06 21:22:46,369 - train - INFO - True
2024-04-06 21:22:46,370 - train - INFO - alphas:tensor([0.2630, 0.2454, 0.1623, 0.1640, 0.1653], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,391 - train - INFO - tau:0.99
2024-04-06 21:22:46,391 - train - INFO - True
2024-04-06 21:22:46,392 - train - INFO - alphas:tensor([0.2649, 0.2314, 0.1677, 0.1678, 0.1682], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,411 - train - INFO - tau:0.99
2024-04-06 21:22:46,411 - train - INFO - True
2024-04-06 21:22:46,415 - train - INFO - alphas:tensor([0.2705, 0.2508, 0.1590, 0.1596, 0.1602], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,433 - train - INFO - tau:0.99
2024-04-06 21:22:46,434 - train - INFO - True
2024-04-06 21:22:46,437 - train - INFO - alphas:tensor([0.2649, 0.2379, 0.1698, 0.1643, 0.1631], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,457 - train - INFO - tau:0.99
2024-04-06 21:22:46,458 - train - INFO - True
2024-04-06 21:22:46,458 - train - INFO - alphas:tensor([0.2726, 0.2502, 0.1579, 0.1588, 0.1606], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,479 - train - INFO - tau:0.99
2024-04-06 21:22:46,479 - train - INFO - True
2024-04-06 21:22:46,484 - train - INFO - alphas:tensor([0.2726, 0.2328, 0.1657, 0.1642, 0.1646], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,503 - train - INFO - tau:0.99
2024-04-06 21:22:46,503 - train - INFO - True
2024-04-06 21:22:46,507 - train - INFO - alphas:tensor([0.2736, 0.2425, 0.1607, 0.1602, 0.1631], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,524 - train - INFO - tau:0.99
2024-04-06 21:22:46,524 - train - INFO - True
2024-04-06 21:22:46,525 - train - INFO - alphas:tensor([0.2617, 0.2219, 0.1743, 0.1725, 0.1695], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,542 - train - INFO - tau:0.99
2024-04-06 21:22:46,542 - train - INFO - True
2024-04-06 21:22:46,543 - train - INFO - alphas:tensor([0.2668, 0.2447, 0.1613, 0.1631, 0.1641], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,560 - train - INFO - tau:0.99
2024-04-06 21:22:46,560 - train - INFO - True
2024-04-06 21:22:46,560 - train - INFO - alphas:tensor([0.2751, 0.2230, 0.1684, 0.1663, 0.1671], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,581 - train - INFO - tau:0.99
2024-04-06 21:22:46,581 - train - INFO - True
2024-04-06 21:22:46,582 - train - INFO - alphas:tensor([0.2692, 0.2361, 0.1687, 0.1630, 0.1631], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,602 - train - INFO - tau:0.99
2024-04-06 21:22:46,602 - train - INFO - True
2024-04-06 21:22:46,603 - train - INFO - alphas:tensor([0.2524, 0.2236, 0.1769, 0.1722, 0.1749], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,622 - train - INFO - tau:0.99
2024-04-06 21:22:46,622 - train - INFO - True
2024-04-06 21:22:46,624 - train - INFO - alphas:tensor([0.2584, 0.2387, 0.1661, 0.1677, 0.1691], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,642 - train - INFO - tau:0.99
2024-04-06 21:22:46,642 - train - INFO - True
2024-04-06 21:22:46,643 - train - INFO - alphas:tensor([0.2707, 0.2217, 0.1698, 0.1689, 0.1689], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,660 - train - INFO - tau:0.99
2024-04-06 21:22:46,660 - train - INFO - True
2024-04-06 21:22:46,662 - train - INFO - alphas:tensor([0.2674, 0.2377, 0.1678, 0.1641, 0.1631], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,679 - train - INFO - tau:0.99
2024-04-06 21:22:46,679 - train - INFO - True
2024-04-06 21:22:46,682 - train - INFO - alphas:tensor([0.2528, 0.2198, 0.1785, 0.1741, 0.1749], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,702 - train - INFO - tau:0.99
2024-04-06 21:22:46,703 - train - INFO - True
2024-04-06 21:22:46,708 - train - INFO - alphas:tensor([0.2557, 0.2326, 0.1689, 0.1708, 0.1721], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,728 - train - INFO - tau:0.99
2024-04-06 21:22:46,728 - train - INFO - True
2024-04-06 21:22:46,729 - train - INFO - alphas:tensor([0.2746, 0.2190, 0.1683, 0.1687, 0.1694], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,748 - train - INFO - tau:0.99
2024-04-06 21:22:46,748 - train - INFO - True
2024-04-06 21:22:46,748 - train - INFO - alphas:tensor([0.2727, 0.2335, 0.1655, 0.1639, 0.1643], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,766 - train - INFO - tau:0.99
2024-04-06 21:22:46,766 - train - INFO - True
2024-04-06 21:22:46,767 - train - INFO - alphas:tensor([0.2414, 0.2144, 0.1834, 0.1801, 0.1808], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,784 - train - INFO - tau:0.99
2024-04-06 21:22:46,784 - train - INFO - True
2024-04-06 21:22:46,785 - train - INFO - alphas:tensor([0.2539, 0.2324, 0.1682, 0.1723, 0.1732], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,802 - train - INFO - tau:0.99
2024-04-06 21:22:46,802 - train - INFO - True
2024-04-06 21:22:46,802 - train - INFO - alphas:tensor([0.2696, 0.2185, 0.1718, 0.1687, 0.1713], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,822 - train - INFO - tau:0.99
2024-04-06 21:22:46,822 - train - INFO - True
2024-04-06 21:22:46,823 - train - INFO - alphas:tensor([0.2827, 0.2387, 0.1624, 0.1590, 0.1572], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,843 - train - INFO - tau:0.99
2024-04-06 21:22:46,843 - train - INFO - True
2024-04-06 21:22:46,848 - train - INFO - alphas:tensor([0.2558, 0.2205, 0.1776, 0.1735, 0.1726], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,867 - train - INFO - tau:0.99
2024-04-06 21:22:46,867 - train - INFO - True
2024-04-06 21:22:46,869 - train - INFO - alphas:tensor([0.2534, 0.2340, 0.1692, 0.1713, 0.1721], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,887 - train - INFO - tau:0.99
2024-04-06 21:22:46,887 - train - INFO - True
2024-04-06 21:22:46,891 - train - INFO - alphas:tensor([0.2680, 0.2261, 0.1687, 0.1675, 0.1698], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,908 - train - INFO - tau:0.99
2024-04-06 21:22:46,908 - train - INFO - True
2024-04-06 21:22:46,912 - train - INFO - alphas:tensor([0.2853, 0.2486, 0.1571, 0.1545, 0.1546], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,931 - train - INFO - tau:0.99
2024-04-06 21:22:46,931 - train - INFO - True
2024-04-06 21:22:46,932 - train - INFO - alphas:tensor([0.2647, 0.2287, 0.1721, 0.1670, 0.1675], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,953 - train - INFO - tau:0.99
2024-04-06 21:22:46,953 - train - INFO - True
2024-04-06 21:22:46,954 - train - INFO - alphas:tensor([0.2454, 0.2353, 0.1713, 0.1732, 0.1748], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,973 - train - INFO - tau:0.99
2024-04-06 21:22:46,973 - train - INFO - True
2024-04-06 21:22:46,974 - train - INFO - alphas:tensor([0.2493, 0.2227, 0.1752, 0.1762, 0.1766], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:46,992 - train - INFO - tau:0.99
2024-04-06 21:22:46,992 - train - INFO - True
2024-04-06 21:22:46,993 - train - INFO - alphas:tensor([0.2811, 0.2506, 0.1556, 0.1562, 0.1565], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:47,010 - train - INFO - tau:0.99
2024-04-06 21:22:47,011 - train - INFO - True
2024-04-06 21:22:47,011 - train - INFO - alphas:tensor([0.2633, 0.2351, 0.1667, 0.1668, 0.1680], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:47,028 - train - INFO - tau:0.99
2024-04-06 21:22:47,028 - train - INFO - True
2024-04-06 21:22:47,029 - train - INFO - alphas:tensor([0.3413, 0.2257, 0.2142, 0.2188], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 21:22:47,045 - train - INFO - tau:0.99
2024-04-06 21:22:47,045 - train - INFO - avg block size:1.0
2024-04-06 21:22:47,046 - train - INFO - current latency ratio:tensor(1.)
2024-04-06 21:22:47,364 - train - INFO - Test: [   0/78]  Time: 0.315 (0.315)  Loss:  1.9346 (1.9346)  Acc@1: 61.7188 (61.7188)  Acc@5: 81.2500 (81.2500)
2024-04-06 21:22:54,166 - train - INFO - Test: [  50/78]  Time: 0.089 (0.140)  Loss:  2.7832 (3.0016)  Acc@1: 47.6562 (37.1630)  Acc@5: 68.7500 (63.5417)
2024-04-06 21:22:57,861 - train - INFO - Test: [  78/78]  Time: 0.140 (0.137)  Loss:  3.0801 (2.9861)  Acc@1: 25.0000 (37.2500)  Acc@5: 56.2500 (63.8200)
2024-04-06 21:22:59,270 - train - INFO - Train: 4 [   0/781 (  0%)]  Loss:  4.607831 (4.6078)  Time: 1.268s,  100.93/s  (1.268s,  100.93/s)  LR: 2.006e-04  Data: 0.192 (0.192)
2024-04-06 21:23:52,620 - train - INFO - Train: 4 [  50/781 (  6%)]  Loss:  4.700283 (4.4639)  Time: 0.921s,  138.98/s  (1.071s,  119.52/s)  LR: 2.006e-04  Data: 0.008 (0.011)
2024-04-06 21:24:46,498 - train - INFO - Train: 4 [ 100/781 ( 13%)]  Loss:  4.044246 (4.4513)  Time: 0.949s,  134.94/s  (1.074s,  119.16/s)  LR: 2.006e-04  Data: 0.007 (0.009)
2024-04-06 21:25:37,862 - train - INFO - Train: 4 [ 150/781 ( 19%)]  Loss:  4.279052 (4.4489)  Time: 0.924s,  138.60/s  (1.059s,  120.91/s)  LR: 2.006e-04  Data: 0.007 (0.009)
