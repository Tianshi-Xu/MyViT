2024-04-06 20:44:33,323 - train - INFO - Training with a single process on 1 GPUs.
2024-04-06 20:44:38,378 - train - INFO - Model vit_7_4_32 created, param count:3717016
2024-04-06 20:44:38,398 - train - INFO - Using native Torch AMP. Training in mixed precision.
2024-04-06 20:44:38,398 - train - INFO - Scheduled epochs: 160
2024-04-06 20:44:39,809 - train - INFO - Verifying teacher model
2024-04-06 20:44:41,512 - train - INFO - Test: [   0/39]  Time: 1.702 (1.702)  Loss:  0.3433 (0.3433)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)
2024-04-06 20:44:42,189 - train - INFO - Test: [  39/39]  Time: 0.041 (0.059)  Loss:  0.3740 (0.3449)  Acc@1: 93.7500 (93.5400)  Acc@5: 100.0000 (99.7600)
2024-04-06 20:44:42,190 - train - INFO - Verifying initial model
2024-04-06 20:44:42,400 - train - INFO - Test: [   0/39]  Time: 0.207 (0.207)  Loss:  2.2070 (2.2070)  Acc@1: 19.1406 (19.1406)  Acc@5: 66.4062 (66.4062)
2024-04-06 20:44:44,705 - train - INFO - Test: [  39/39]  Time: 0.060 (0.063)  Loss:  2.1387 (2.2048)  Acc@1: 31.2500 (20.1100)  Acc@5: 75.0000 (65.0300)
2024-04-06 20:44:46,826 - train - INFO - Train: 0 [   0/195 (  0%)]  Loss:  2.331990 (2.3320)  Time: 2.117s,  120.92/s  (2.117s,  120.92/s)  LR: 1.000e-05  Data: 0.537 (0.537)
2024-04-06 20:45:31,095 - train - INFO - Train: 0 [  50/195 ( 26%)]  Loss:  2.309395 (2.3248)  Time: 0.779s,  328.83/s  (0.909s,  281.47/s)  LR: 1.000e-05  Data: 0.007 (0.020)
2024-04-06 20:46:12,282 - train - INFO - Train: 0 [ 100/195 ( 52%)]  Loss:  2.314361 (2.3172)  Time: 0.918s,  278.85/s  (0.867s,  295.26/s)  LR: 1.000e-05  Data: 0.007 (0.014)
2024-04-06 20:46:58,116 - train - INFO - Train: 0 [ 150/195 ( 77%)]  Loss:  2.308108 (2.3102)  Time: 1.122s,  228.25/s  (0.883s,  289.77/s)  LR: 1.000e-05  Data: 0.012 (0.013)
2024-04-06 20:47:34,797 - train - INFO - Train: 0 [ 194/195 (100%)]  Loss:  2.274277 (2.3063)  Time: 0.791s,  323.80/s  (0.872s,  293.51/s)  LR: 1.000e-05  Data: 0.000 (0.012)
2024-04-06 20:47:34,797 - train - INFO - True
2024-04-06 20:47:34,803 - train - INFO - alphas:tensor([0.2001, 0.2002, 0.1999, 0.1999, 0.1998], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:34,834 - train - INFO - True
2024-04-06 20:47:34,835 - train - INFO - alphas:tensor([0.2004, 0.2004, 0.1998, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:34,863 - train - INFO - True
2024-04-06 20:47:34,864 - train - INFO - alphas:tensor([0.2005, 0.2004, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:34,903 - train - INFO - True
2024-04-06 20:47:34,904 - train - INFO - alphas:tensor([0.1999, 0.2002, 0.2000, 0.2000, 0.1999], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:34,926 - train - INFO - True
2024-04-06 20:47:34,926 - train - INFO - alphas:tensor([0.2002, 0.2003, 0.1999, 0.1998, 0.1998], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:34,947 - train - INFO - True
2024-04-06 20:47:34,948 - train - INFO - alphas:tensor([0.2004, 0.2004, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:34,967 - train - INFO - True
2024-04-06 20:47:34,968 - train - INFO - alphas:tensor([0.2005, 0.2005, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:34,997 - train - INFO - True
2024-04-06 20:47:34,998 - train - INFO - alphas:tensor([0.2004, 0.2002, 0.1998, 0.1997, 0.1998], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,028 - train - INFO - True
2024-04-06 20:47:35,028 - train - INFO - alphas:tensor([0.2004, 0.2004, 0.1998, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,058 - train - INFO - True
2024-04-06 20:47:35,059 - train - INFO - alphas:tensor([0.2004, 0.2005, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,077 - train - INFO - True
2024-04-06 20:47:35,078 - train - INFO - alphas:tensor([0.2005, 0.2005, 0.1997, 0.1997, 0.1996], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,107 - train - INFO - True
2024-04-06 20:47:35,108 - train - INFO - alphas:tensor([0.2002, 0.2000, 0.2001, 0.1999, 0.1998], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,138 - train - INFO - True
2024-04-06 20:47:35,138 - train - INFO - alphas:tensor([0.2005, 0.2005, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,168 - train - INFO - True
2024-04-06 20:47:35,169 - train - INFO - alphas:tensor([0.2005, 0.2005, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,198 - train - INFO - True
2024-04-06 20:47:35,199 - train - INFO - alphas:tensor([0.2003, 0.2004, 0.1998, 0.1998, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,217 - train - INFO - True
2024-04-06 20:47:35,218 - train - INFO - alphas:tensor([0.1997, 0.1997, 0.2002, 0.2002, 0.2002], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,234 - train - INFO - True
2024-04-06 20:47:35,235 - train - INFO - alphas:tensor([0.2005, 0.2005, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,253 - train - INFO - True
2024-04-06 20:47:35,254 - train - INFO - alphas:tensor([0.2005, 0.2005, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,283 - train - INFO - True
2024-04-06 20:47:35,283 - train - INFO - alphas:tensor([0.2004, 0.2004, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,313 - train - INFO - True
2024-04-06 20:47:35,313 - train - INFO - alphas:tensor([0.1996, 0.1996, 0.2002, 0.2003, 0.2003], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,328 - train - INFO - True
2024-04-06 20:47:35,329 - train - INFO - alphas:tensor([0.2005, 0.2004, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,358 - train - INFO - True
2024-04-06 20:47:35,359 - train - INFO - alphas:tensor([0.2005, 0.2004, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,388 - train - INFO - True
2024-04-06 20:47:35,388 - train - INFO - alphas:tensor([0.2004, 0.2004, 0.1998, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,418 - train - INFO - True
2024-04-06 20:47:35,419 - train - INFO - alphas:tensor([0.1999, 0.1997, 0.1999, 0.2003, 0.2002], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,434 - train - INFO - True
2024-04-06 20:47:35,434 - train - INFO - alphas:tensor([0.2005, 0.2005, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,453 - train - INFO - True
2024-04-06 20:47:35,454 - train - INFO - alphas:tensor([0.2005, 0.2005, 0.1997, 0.1997, 0.1997], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,483 - train - INFO - True
2024-04-06 20:47:35,483 - train - INFO - alphas:tensor([0.1998, 0.1998, 0.2000, 0.2002, 0.2001], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,498 - train - INFO - True
2024-04-06 20:47:35,499 - train - INFO - alphas:tensor([0.2000, 0.1998, 0.2000, 0.2001, 0.2001], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,514 - train - INFO - True
2024-04-06 20:47:35,515 - train - INFO - alphas:tensor([0.5010, 0.4990], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-04-06 20:47:35,515 - train - INFO - avg block size:2.9310344827586206
2024-04-06 20:47:35,515 - train - INFO - current latency ratio:tensor(0.6871)
2024-04-06 20:47:35,705 - train - INFO - Test: [   0/39]  Time: 0.187 (0.187)  Loss:  2.0801 (2.0801)  Acc@1: 35.5469 (35.5469)  Acc@5: 82.0312 (82.0312)
2024-04-06 20:47:37,622 - train - INFO - Test: [  39/39]  Time: 0.040 (0.053)  Loss:  1.9912 (2.0739)  Acc@1: 43.7500 (33.0000)  Acc@5: 93.7500 (84.6100)
2024-04-06 20:47:38,960 - train - INFO - Train: 1 [   0/195 (  0%)]  Loss:  2.283766 (2.2838)  Time: 1.266s,  202.24/s  (1.266s,  202.24/s)  LR: 6.400e-05  Data: 0.204 (0.204)
2024-04-06 20:48:21,036 - train - INFO - Train: 1 [  50/195 ( 26%)]  Loss:  2.288146 (2.2700)  Time: 0.816s,  313.66/s  (0.850s,  301.24/s)  LR: 6.400e-05  Data: 0.008 (0.013)
2024-04-06 20:49:05,366 - train - INFO - Train: 1 [ 100/195 ( 52%)]  Loss:  2.247811 (2.2385)  Time: 0.929s,  275.70/s  (0.868s,  294.93/s)  LR: 6.400e-05  Data: 0.006 (0.011)
2024-04-06 20:49:50,902 - train - INFO - Train: 1 [ 150/195 ( 77%)]  Loss:  2.177918 (2.2058)  Time: 0.837s,  305.91/s  (0.882s,  290.20/s)  LR: 6.400e-05  Data: 0.006 (0.010)
2024-04-06 20:50:30,884 - train - INFO - Train: 1 [ 194/195 (100%)]  Loss:  1.987635 (2.1881)  Time: 0.898s,  285.03/s  (0.888s,  288.25/s)  LR: 6.400e-05  Data: 0.000 (0.009)
2024-04-06 20:50:30,885 - train - INFO - True
2024-04-06 20:50:30,886 - train - INFO - alphas:tensor([0.1989, 0.2000, 0.2008, 0.2004, 0.2000], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:30,907 - train - INFO - True
2024-04-06 20:50:30,908 - train - INFO - alphas:tensor([0.2011, 0.2010, 0.1992, 0.1993, 0.1993], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:30,946 - train - INFO - True
2024-04-06 20:50:30,947 - train - INFO - alphas:tensor([0.2055, 0.2050, 0.1966, 0.1965, 0.1964], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:30,985 - train - INFO - True
2024-04-06 20:50:30,986 - train - INFO - alphas:tensor([0.2045, 0.2030, 0.1984, 0.1971, 0.1970], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,024 - train - INFO - True
2024-04-06 20:50:31,025 - train - INFO - alphas:tensor([0.2035, 0.2035, 0.1979, 0.1976, 0.1975], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,049 - train - INFO - True
2024-04-06 20:50:31,050 - train - INFO - alphas:tensor([0.2031, 0.2027, 0.1984, 0.1979, 0.1979], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,089 - train - INFO - True
2024-04-06 20:50:31,090 - train - INFO - alphas:tensor([0.2050, 0.2045, 0.1968, 0.1968, 0.1968], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,128 - train - INFO - True
2024-04-06 20:50:31,129 - train - INFO - alphas:tensor([0.2048, 0.2044, 0.1969, 0.1968, 0.1971], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,167 - train - INFO - True
2024-04-06 20:50:31,168 - train - INFO - alphas:tensor([0.2042, 0.2043, 0.1971, 0.1972, 0.1972], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,192 - train - INFO - True
2024-04-06 20:50:31,193 - train - INFO - alphas:tensor([0.2036, 0.2031, 0.1981, 0.1977, 0.1976], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,231 - train - INFO - True
2024-04-06 20:50:31,232 - train - INFO - alphas:tensor([0.2050, 0.2042, 0.1971, 0.1969, 0.1968], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,270 - train - INFO - True
2024-04-06 20:50:31,271 - train - INFO - alphas:tensor([0.2048, 0.2025, 0.1984, 0.1972, 0.1972], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,309 - train - INFO - True
2024-04-06 20:50:31,310 - train - INFO - alphas:tensor([0.2045, 0.2044, 0.1971, 0.1970, 0.1969], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,348 - train - INFO - True
2024-04-06 20:50:31,349 - train - INFO - alphas:tensor([0.2040, 0.2041, 0.1975, 0.1972, 0.1973], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,373 - train - INFO - True
2024-04-06 20:50:31,374 - train - INFO - alphas:tensor([0.2043, 0.2038, 0.1972, 0.1974, 0.1973], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,412 - train - INFO - True
2024-04-06 20:50:31,413 - train - INFO - alphas:tensor([0.2000, 0.1996, 0.2003, 0.2000, 0.2001], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,434 - train - INFO - True
2024-04-06 20:50:31,435 - train - INFO - alphas:tensor([0.2047, 0.2049, 0.1968, 0.1968, 0.1967], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,459 - train - INFO - True
2024-04-06 20:50:31,460 - train - INFO - alphas:tensor([0.2041, 0.2039, 0.1973, 0.1974, 0.1973], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,499 - train - INFO - True
2024-04-06 20:50:31,500 - train - INFO - alphas:tensor([0.2029, 0.2027, 0.1982, 0.1981, 0.1980], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,538 - train - INFO - True
2024-04-06 20:50:31,539 - train - INFO - alphas:tensor([0.1984, 0.1984, 0.2007, 0.2012, 0.2012], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,558 - train - INFO - True
2024-04-06 20:50:31,559 - train - INFO - alphas:tensor([0.2040, 0.2043, 0.1974, 0.1972, 0.1972], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,584 - train - INFO - True
2024-04-06 20:50:31,585 - train - INFO - alphas:tensor([0.2035, 0.2034, 0.1978, 0.1977, 0.1976], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,623 - train - INFO - True
2024-04-06 20:50:31,624 - train - INFO - alphas:tensor([0.2023, 0.2019, 0.1986, 0.1986, 0.1985], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,662 - train - INFO - True
2024-04-06 20:50:31,663 - train - INFO - alphas:tensor([0.2015, 0.1987, 0.1996, 0.2001, 0.2000], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,701 - train - INFO - True
2024-04-06 20:50:31,702 - train - INFO - alphas:tensor([0.2038, 0.2038, 0.1976, 0.1974, 0.1974], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,726 - train - INFO - True
2024-04-06 20:50:31,727 - train - INFO - alphas:tensor([0.2031, 0.2033, 0.1980, 0.1978, 0.1978], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,751 - train - INFO - True
2024-04-06 20:50:31,752 - train - INFO - alphas:tensor([0.2009, 0.2010, 0.1996, 0.1993, 0.1992], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,776 - train - INFO - True
2024-04-06 20:50:31,777 - train - INFO - alphas:tensor([0.2031, 0.2012, 0.1989, 0.1984, 0.1984], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,815 - train - INFO - True
2024-04-06 20:50:31,815 - train - INFO - alphas:tensor([0.5064, 0.4936], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-04-06 20:50:31,816 - train - INFO - avg block size:1.7241379310344827
2024-04-06 20:50:31,816 - train - INFO - current latency ratio:tensor(0.7477)
2024-04-06 20:50:31,922 - train - INFO - Test: [   0/39]  Time: 0.104 (0.104)  Loss:  1.5801 (1.5801)  Acc@1: 48.4375 (48.4375)  Acc@5: 91.0156 (91.0156)
2024-04-06 20:50:34,350 - train - INFO - Test: [  39/39]  Time: 0.047 (0.063)  Loss:  1.5186 (1.5440)  Acc@1: 56.2500 (52.6500)  Acc@5: 87.5000 (93.5300)
2024-04-06 20:50:35,501 - train - INFO - Train: 2 [   0/195 (  0%)]  Loss:  2.134467 (2.1345)  Time: 1.064s,  240.62/s  (1.064s,  240.62/s)  LR: 1.180e-04  Data: 0.164 (0.164)
2024-04-06 20:51:22,850 - train - INFO - Train: 2 [  50/195 ( 26%)]  Loss:  2.220831 (2.0886)  Time: 0.904s,  283.22/s  (0.949s,  269.68/s)  LR: 1.180e-04  Data: 0.006 (0.011)
2024-04-06 20:52:11,320 - train - INFO - Train: 2 [ 100/195 ( 52%)]  Loss:  2.148127 (2.0701)  Time: 1.015s,  252.33/s  (0.959s,  266.88/s)  LR: 1.180e-04  Data: 0.005 (0.010)
2024-04-06 20:53:01,804 - train - INFO - Train: 2 [ 150/195 ( 77%)]  Loss:  1.961106 (2.0575)  Time: 0.952s,  268.91/s  (0.976s,  262.32/s)  LR: 1.180e-04  Data: 0.014 (0.009)
2024-04-06 20:53:46,851 - train - INFO - Train: 2 [ 194/195 (100%)]  Loss:  2.094235 (2.0474)  Time: 0.916s,  279.55/s  (0.987s,  259.45/s)  LR: 1.180e-04  Data: 0.000 (0.009)
2024-04-06 20:53:46,852 - train - INFO - True
2024-04-06 20:53:46,853 - train - INFO - alphas:tensor([0.2005, 0.2027, 0.2000, 0.1987, 0.1981], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:46,905 - train - INFO - True
2024-04-06 20:53:46,906 - train - INFO - alphas:tensor([0.2044, 0.2037, 0.1970, 0.1975, 0.1974], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:46,959 - train - INFO - True
2024-04-06 20:53:46,960 - train - INFO - alphas:tensor([0.2139, 0.2118, 0.1917, 0.1913, 0.1912], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,017 - train - INFO - True
2024-04-06 20:53:47,018 - train - INFO - alphas:tensor([0.2124, 0.2073, 0.1942, 0.1927, 0.1932], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,056 - train - INFO - True
2024-04-06 20:53:47,057 - train - INFO - alphas:tensor([0.2092, 0.2085, 0.1946, 0.1940, 0.1938], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,095 - train - INFO - True
2024-04-06 20:53:47,096 - train - INFO - alphas:tensor([0.2081, 0.2060, 0.1965, 0.1948, 0.1946], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,134 - train - INFO - True
2024-04-06 20:53:47,135 - train - INFO - alphas:tensor([0.2118, 0.2097, 0.1924, 0.1931, 0.1930], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,173 - train - INFO - True
2024-04-06 20:53:47,174 - train - INFO - alphas:tensor([0.2111, 0.2092, 0.1928, 0.1928, 0.1941], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,212 - train - INFO - True
2024-04-06 20:53:47,213 - train - INFO - alphas:tensor([0.2079, 0.2075, 0.1943, 0.1951, 0.1952], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,251 - train - INFO - True
2024-04-06 20:53:47,252 - train - INFO - alphas:tensor([0.2072, 0.2058, 0.1962, 0.1955, 0.1954], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,291 - train - INFO - True
2024-04-06 20:53:47,292 - train - INFO - alphas:tensor([0.2113, 0.2075, 0.1933, 0.1939, 0.1940], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,344 - train - INFO - True
2024-04-06 20:53:47,345 - train - INFO - alphas:tensor([0.2103, 0.2057, 0.1952, 0.1942, 0.1945], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,383 - train - INFO - True
2024-04-06 20:53:47,385 - train - INFO - alphas:tensor([0.2084, 0.2076, 0.1945, 0.1947, 0.1947], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,423 - train - INFO - True
2024-04-06 20:53:47,424 - train - INFO - alphas:tensor([0.2081, 0.2076, 0.1946, 0.1947, 0.1950], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,462 - train - INFO - True
2024-04-06 20:53:47,464 - train - INFO - alphas:tensor([0.2101, 0.2079, 0.1930, 0.1944, 0.1946], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,501 - train - INFO - True
2024-04-06 20:53:47,502 - train - INFO - alphas:tensor([0.2037, 0.2009, 0.1988, 0.1981, 0.1986], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,540 - train - INFO - True
2024-04-06 20:53:47,541 - train - INFO - alphas:tensor([0.2079, 0.2080, 0.1942, 0.1949, 0.1950], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,565 - train - INFO - True
2024-04-06 20:53:47,566 - train - INFO - alphas:tensor([0.2076, 0.2066, 0.1949, 0.1955, 0.1954], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,604 - train - INFO - True
2024-04-06 20:53:47,605 - train - INFO - alphas:tensor([0.2064, 0.2053, 0.1960, 0.1961, 0.1962], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,643 - train - INFO - True
2024-04-06 20:53:47,644 - train - INFO - alphas:tensor([0.1988, 0.1983, 0.2003, 0.2012, 0.2013], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,663 - train - INFO - True
2024-04-06 20:53:47,664 - train - INFO - alphas:tensor([0.2071, 0.2069, 0.1954, 0.1953, 0.1954], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,703 - train - INFO - True
2024-04-06 20:53:47,704 - train - INFO - alphas:tensor([0.2069, 0.2067, 0.1956, 0.1954, 0.1954], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,742 - train - INFO - True
2024-04-06 20:53:47,743 - train - INFO - alphas:tensor([0.2049, 0.2037, 0.1970, 0.1973, 0.1971], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,781 - train - INFO - True
2024-04-06 20:53:47,782 - train - INFO - alphas:tensor([0.2030, 0.1985, 0.1996, 0.1993, 0.1996], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,820 - train - INFO - True
2024-04-06 20:53:47,822 - train - INFO - alphas:tensor([0.2071, 0.2067, 0.1954, 0.1953, 0.1955], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,860 - train - INFO - True
2024-04-06 20:53:47,862 - train - INFO - alphas:tensor([0.2060, 0.2062, 0.1961, 0.1958, 0.1959], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,886 - train - INFO - True
2024-04-06 20:53:47,887 - train - INFO - alphas:tensor([0.2022, 0.2020, 0.1987, 0.1986, 0.1986], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,925 - train - INFO - True
2024-04-06 20:53:47,926 - train - INFO - alphas:tensor([0.2037, 0.2015, 0.1985, 0.1981, 0.1981], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,965 - train - INFO - True
2024-04-06 20:53:47,966 - train - INFO - alphas:tensor([0.5107, 0.4893], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-04-06 20:53:47,966 - train - INFO - avg block size:1.6206896551724137
2024-04-06 20:53:47,967 - train - INFO - current latency ratio:tensor(0.9080)
2024-04-06 20:53:48,077 - train - INFO - Test: [   0/39]  Time: 0.106 (0.106)  Loss:  1.2979 (1.2979)  Acc@1: 58.5938 (58.5938)  Acc@5: 94.9219 (94.9219)
2024-04-06 20:53:51,221 - train - INFO - Test: [  39/39]  Time: 0.077 (0.081)  Loss:  1.3486 (1.2736)  Acc@1: 56.2500 (59.7500)  Acc@5: 100.0000 (95.7400)
2024-04-06 20:53:52,608 - train - INFO - Train: 3 [   0/195 (  0%)]  Loss:  1.967740 (1.9677)  Time: 1.297s,  197.35/s  (1.297s,  197.35/s)  LR: 1.720e-04  Data: 0.212 (0.212)
2024-04-06 20:54:41,848 - train - INFO - Train: 3 [  50/195 ( 26%)]  Loss:  2.022012 (2.0080)  Time: 0.924s,  277.20/s  (0.991s,  258.35/s)  LR: 1.720e-04  Data: 0.009 (0.013)
2024-04-06 20:55:26,135 - train - INFO - Train: 3 [ 100/195 ( 52%)]  Loss:  1.991636 (1.9894)  Time: 0.809s,  316.39/s  (0.939s,  272.68/s)  LR: 1.720e-04  Data: 0.008 (0.011)
2024-04-06 20:56:07,719 - train - INFO - Train: 3 [ 150/195 ( 77%)]  Loss:  2.090109 (1.9883)  Time: 0.805s,  317.83/s  (0.903s,  283.39/s)  LR: 1.720e-04  Data: 0.007 (0.010)
2024-04-06 20:56:43,545 - train - INFO - Train: 3 [ 194/195 (100%)]  Loss:  1.876030 (1.9859)  Time: 0.804s,  318.37/s  (0.883s,  289.85/s)  LR: 1.720e-04  Data: 0.000 (0.010)
2024-04-06 20:56:43,545 - train - INFO - True
2024-04-06 20:56:43,547 - train - INFO - alphas:tensor([0.2042, 0.2074, 0.1977, 0.1956, 0.1951], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,577 - train - INFO - tau:0.99
2024-04-06 20:56:43,577 - train - INFO - True
2024-04-06 20:56:43,578 - train - INFO - alphas:tensor([0.2088, 0.2072, 0.1942, 0.1949, 0.1948], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,620 - train - INFO - tau:0.99
2024-04-06 20:56:43,620 - train - INFO - True
2024-04-06 20:56:43,621 - train - INFO - alphas:tensor([0.2243, 0.2198, 0.1854, 0.1853, 0.1852], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,657 - train - INFO - tau:0.99
2024-04-06 20:56:43,657 - train - INFO - True
2024-04-06 20:56:43,658 - train - INFO - alphas:tensor([0.2227, 0.2124, 0.1889, 0.1874, 0.1886], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,691 - train - INFO - tau:0.99
2024-04-06 20:56:43,691 - train - INFO - True
2024-04-06 20:56:43,692 - train - INFO - alphas:tensor([0.2157, 0.2143, 0.1903, 0.1899, 0.1898], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,722 - train - INFO - tau:0.99
2024-04-06 20:56:43,722 - train - INFO - True
2024-04-06 20:56:43,723 - train - INFO - alphas:tensor([0.2152, 0.2101, 0.1937, 0.1906, 0.1905], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,752 - train - INFO - tau:0.99
2024-04-06 20:56:43,752 - train - INFO - True
2024-04-06 20:56:43,753 - train - INFO - alphas:tensor([0.2195, 0.2156, 0.1873, 0.1890, 0.1887], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,782 - train - INFO - tau:0.99
2024-04-06 20:56:43,782 - train - INFO - True
2024-04-06 20:56:43,783 - train - INFO - alphas:tensor([0.2188, 0.2141, 0.1884, 0.1882, 0.1905], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,812 - train - INFO - tau:0.99
2024-04-06 20:56:43,812 - train - INFO - True
2024-04-06 20:56:43,813 - train - INFO - alphas:tensor([0.2136, 0.2125, 0.1898, 0.1918, 0.1923], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,843 - train - INFO - tau:0.99
2024-04-06 20:56:43,843 - train - INFO - True
2024-04-06 20:56:43,843 - train - INFO - alphas:tensor([0.2129, 0.2099, 0.1931, 0.1922, 0.1919], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,873 - train - INFO - tau:0.99
2024-04-06 20:56:43,873 - train - INFO - True
2024-04-06 20:56:43,874 - train - INFO - alphas:tensor([0.2189, 0.2111, 0.1885, 0.1905, 0.1910], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,903 - train - INFO - tau:0.99
2024-04-06 20:56:43,903 - train - INFO - True
2024-04-06 20:56:43,904 - train - INFO - alphas:tensor([0.2167, 0.2088, 0.1922, 0.1908, 0.1915], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,933 - train - INFO - tau:0.99
2024-04-06 20:56:43,934 - train - INFO - True
2024-04-06 20:56:43,934 - train - INFO - alphas:tensor([0.2122, 0.2104, 0.1921, 0.1925, 0.1928], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,964 - train - INFO - tau:0.99
2024-04-06 20:56:43,964 - train - INFO - True
2024-04-06 20:56:43,964 - train - INFO - alphas:tensor([0.2123, 0.2115, 0.1913, 0.1923, 0.1926], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:43,994 - train - INFO - tau:0.99
2024-04-06 20:56:43,994 - train - INFO - True
2024-04-06 20:56:43,995 - train - INFO - alphas:tensor([0.2180, 0.2133, 0.1874, 0.1904, 0.1909], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,024 - train - INFO - tau:0.99
2024-04-06 20:56:44,024 - train - INFO - True
2024-04-06 20:56:44,025 - train - INFO - alphas:tensor([0.2089, 0.2026, 0.1970, 0.1954, 0.1961], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,055 - train - INFO - tau:0.99
2024-04-06 20:56:44,055 - train - INFO - True
2024-04-06 20:56:44,055 - train - INFO - alphas:tensor([0.2108, 0.2105, 0.1920, 0.1932, 0.1935], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,085 - train - INFO - tau:0.99
2024-04-06 20:56:44,085 - train - INFO - True
2024-04-06 20:56:44,086 - train - INFO - alphas:tensor([0.2112, 0.2096, 0.1926, 0.1933, 0.1933], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,115 - train - INFO - tau:0.99
2024-04-06 20:56:44,115 - train - INFO - True
2024-04-06 20:56:44,116 - train - INFO - alphas:tensor([0.2117, 0.2091, 0.1927, 0.1931, 0.1934], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,145 - train - INFO - tau:0.99
2024-04-06 20:56:44,145 - train - INFO - True
2024-04-06 20:56:44,146 - train - INFO - alphas:tensor([0.2005, 0.1986, 0.1998, 0.2004, 0.2007], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,161 - train - INFO - tau:0.99
2024-04-06 20:56:44,161 - train - INFO - True
2024-04-06 20:56:44,161 - train - INFO - alphas:tensor([0.2098, 0.2087, 0.1936, 0.1939, 0.1940], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,190 - train - INFO - tau:0.99
2024-04-06 20:56:44,190 - train - INFO - True
2024-04-06 20:56:44,191 - train - INFO - alphas:tensor([0.2106, 0.2102, 0.1933, 0.1929, 0.1930], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,220 - train - INFO - tau:0.99
2024-04-06 20:56:44,220 - train - INFO - True
2024-04-06 20:56:44,221 - train - INFO - alphas:tensor([0.2084, 0.2059, 0.1949, 0.1953, 0.1955], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,250 - train - INFO - tau:0.99
2024-04-06 20:56:44,250 - train - INFO - True
2024-04-06 20:56:44,251 - train - INFO - alphas:tensor([0.2049, 0.1989, 0.1995, 0.1981, 0.1986], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,280 - train - INFO - tau:0.99
2024-04-06 20:56:44,280 - train - INFO - True
2024-04-06 20:56:44,281 - train - INFO - alphas:tensor([0.2097, 0.2089, 0.1936, 0.1938, 0.1941], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,310 - train - INFO - tau:0.99
2024-04-06 20:56:44,310 - train - INFO - True
2024-04-06 20:56:44,311 - train - INFO - alphas:tensor([0.2089, 0.2090, 0.1943, 0.1939, 0.1940], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,329 - train - INFO - tau:0.99
2024-04-06 20:56:44,329 - train - INFO - True
2024-04-06 20:56:44,330 - train - INFO - alphas:tensor([0.2044, 0.2035, 0.1970, 0.1975, 0.1976], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,359 - train - INFO - tau:0.99
2024-04-06 20:56:44,359 - train - INFO - True
2024-04-06 20:56:44,359 - train - INFO - alphas:tensor([0.2043, 0.2018, 0.1982, 0.1978, 0.1980], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,388 - train - INFO - tau:0.99
2024-04-06 20:56:44,388 - train - INFO - True
2024-04-06 20:56:44,389 - train - INFO - alphas:tensor([0.5140, 0.4860], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-04-06 20:56:44,389 - train - INFO - tau:0.99
2024-04-06 20:56:44,389 - train - INFO - avg block size:1.5862068965517242
2024-04-06 20:56:44,389 - train - INFO - current latency ratio:tensor(0.9327)
2024-04-06 20:56:44,568 - train - INFO - Test: [   0/39]  Time: 0.176 (0.176)  Loss:  1.1797 (1.1797)  Acc@1: 64.0625 (64.0625)  Acc@5: 96.4844 (96.4844)
2024-04-06 20:56:46,479 - train - INFO - Test: [  39/39]  Time: 0.040 (0.052)  Loss:  1.1924 (1.1343)  Acc@1: 68.7500 (66.2200)  Acc@5: 100.0000 (96.6100)
2024-04-06 20:56:47,600 - train - INFO - Train: 4 [   0/195 (  0%)]  Loss:  1.781892 (1.7819)  Time: 1.054s,  242.81/s  (1.054s,  242.81/s)  LR: 2.260e-04  Data: 0.213 (0.213)
2024-04-06 20:57:28,806 - train - INFO - Train: 4 [  50/195 ( 26%)]  Loss:  1.934010 (1.9392)  Time: 0.804s,  318.41/s  (0.829s,  308.95/s)  LR: 2.260e-04  Data: 0.007 (0.012)
2024-04-06 20:58:10,043 - train - INFO - Train: 4 [ 100/195 ( 52%)]  Loss:  1.973097 (1.9224)  Time: 0.804s,  318.37/s  (0.827s,  309.67/s)  LR: 2.260e-04  Data: 0.008 (0.010)
2024-04-06 20:58:50,751 - train - INFO - Train: 4 [ 150/195 ( 77%)]  Loss:  1.874783 (1.9301)  Time: 0.809s,  316.42/s  (0.823s,  311.23/s)  LR: 2.260e-04  Data: 0.007 (0.009)
2024-04-06 20:59:28,931 - train - INFO - Train: 4 [ 194/195 (100%)]  Loss:  1.714184 (1.9220)  Time: 0.915s,  279.73/s  (0.833s,  307.43/s)  LR: 2.260e-04  Data: 0.000 (0.009)
2024-04-06 20:59:28,931 - train - INFO - True
2024-04-06 20:59:28,933 - train - INFO - alphas:tensor([0.2085, 0.2122, 0.1947, 0.1925, 0.1921], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:28,968 - train - INFO - tau:0.9801
2024-04-06 20:59:28,968 - train - INFO - True
2024-04-06 20:59:28,969 - train - INFO - alphas:tensor([0.2130, 0.2094, 0.1916, 0.1930, 0.1929], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,015 - train - INFO - tau:0.9801
2024-04-06 20:59:29,015 - train - INFO - True
2024-04-06 20:59:29,016 - train - INFO - alphas:tensor([0.2389, 0.2300, 0.1770, 0.1770, 0.1772], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,055 - train - INFO - tau:0.9801
2024-04-06 20:59:29,055 - train - INFO - True
2024-04-06 20:59:29,056 - train - INFO - alphas:tensor([0.2377, 0.2188, 0.1815, 0.1802, 0.1818], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,091 - train - INFO - tau:0.9801
2024-04-06 20:59:29,091 - train - INFO - True
2024-04-06 20:59:29,092 - train - INFO - alphas:tensor([0.2222, 0.2199, 0.1858, 0.1861, 0.1861], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,124 - train - INFO - tau:0.9801
2024-04-06 20:59:29,124 - train - INFO - True
2024-04-06 20:59:29,124 - train - INFO - alphas:tensor([0.2235, 0.2143, 0.1900, 0.1861, 0.1861], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,154 - train - INFO - tau:0.9801
2024-04-06 20:59:29,154 - train - INFO - True
2024-04-06 20:59:29,155 - train - INFO - alphas:tensor([0.2302, 0.2231, 0.1804, 0.1834, 0.1829], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,184 - train - INFO - tau:0.9801
2024-04-06 20:59:29,184 - train - INFO - True
2024-04-06 20:59:29,185 - train - INFO - alphas:tensor([0.2299, 0.2200, 0.1823, 0.1822, 0.1856], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,214 - train - INFO - tau:0.9801
2024-04-06 20:59:29,214 - train - INFO - True
2024-04-06 20:59:29,215 - train - INFO - alphas:tensor([0.2238, 0.2207, 0.1822, 0.1862, 0.1871], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,243 - train - INFO - tau:0.9801
2024-04-06 20:59:29,244 - train - INFO - True
2024-04-06 20:59:29,244 - train - INFO - alphas:tensor([0.2233, 0.2168, 0.1876, 0.1862, 0.1861], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,273 - train - INFO - tau:0.9801
2024-04-06 20:59:29,273 - train - INFO - True
2024-04-06 20:59:29,274 - train - INFO - alphas:tensor([0.2298, 0.2158, 0.1820, 0.1857, 0.1866], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,303 - train - INFO - tau:0.9801
2024-04-06 20:59:29,303 - train - INFO - True
2024-04-06 20:59:29,303 - train - INFO - alphas:tensor([0.2260, 0.2126, 0.1886, 0.1858, 0.1870], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,332 - train - INFO - tau:0.9801
2024-04-06 20:59:29,332 - train - INFO - True
2024-04-06 20:59:29,333 - train - INFO - alphas:tensor([0.2180, 0.2145, 0.1884, 0.1891, 0.1899], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,362 - train - INFO - tau:0.9801
2024-04-06 20:59:29,362 - train - INFO - True
2024-04-06 20:59:29,363 - train - INFO - alphas:tensor([0.2186, 0.2165, 0.1869, 0.1888, 0.1893], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,393 - train - INFO - tau:0.9801
2024-04-06 20:59:29,393 - train - INFO - True
2024-04-06 20:59:29,393 - train - INFO - alphas:tensor([0.2297, 0.2207, 0.1795, 0.1848, 0.1854], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,423 - train - INFO - tau:0.9801
2024-04-06 20:59:29,423 - train - INFO - True
2024-04-06 20:59:29,423 - train - INFO - alphas:tensor([0.2177, 0.2052, 0.1939, 0.1912, 0.1920], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,453 - train - INFO - tau:0.9801
2024-04-06 20:59:29,453 - train - INFO - True
2024-04-06 20:59:29,454 - train - INFO - alphas:tensor([0.2149, 0.2134, 0.1890, 0.1911, 0.1917], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,483 - train - INFO - tau:0.9801
2024-04-06 20:59:29,483 - train - INFO - True
2024-04-06 20:59:29,484 - train - INFO - alphas:tensor([0.2160, 0.2134, 0.1897, 0.1905, 0.1904], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,513 - train - INFO - tau:0.9801
2024-04-06 20:59:29,513 - train - INFO - True
2024-04-06 20:59:29,513 - train - INFO - alphas:tensor([0.2204, 0.2143, 0.1879, 0.1885, 0.1888], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,542 - train - INFO - tau:0.9801
2024-04-06 20:59:29,543 - train - INFO - True
2024-04-06 20:59:29,543 - train - INFO - alphas:tensor([0.2048, 0.1996, 0.1988, 0.1979, 0.1989], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,575 - train - INFO - tau:0.9801
2024-04-06 20:59:29,575 - train - INFO - True
2024-04-06 20:59:29,575 - train - INFO - alphas:tensor([0.2135, 0.2110, 0.1913, 0.1919, 0.1923], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,605 - train - INFO - tau:0.9801
2024-04-06 20:59:29,605 - train - INFO - True
2024-04-06 20:59:29,606 - train - INFO - alphas:tensor([0.2156, 0.2149, 0.1899, 0.1898, 0.1898], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,635 - train - INFO - tau:0.9801
2024-04-06 20:59:29,635 - train - INFO - True
2024-04-06 20:59:29,636 - train - INFO - alphas:tensor([0.2144, 0.2098, 0.1913, 0.1920, 0.1926], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,666 - train - INFO - tau:0.9801
2024-04-06 20:59:29,666 - train - INFO - True
2024-04-06 20:59:29,666 - train - INFO - alphas:tensor([0.2082, 0.2000, 0.1993, 0.1957, 0.1968], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,696 - train - INFO - tau:0.9801
2024-04-06 20:59:29,696 - train - INFO - True
2024-04-06 20:59:29,697 - train - INFO - alphas:tensor([0.2129, 0.2111, 0.1913, 0.1921, 0.1926], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,726 - train - INFO - tau:0.9801
2024-04-06 20:59:29,726 - train - INFO - True
2024-04-06 20:59:29,727 - train - INFO - alphas:tensor([0.2126, 0.2125, 0.1919, 0.1915, 0.1916], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,756 - train - INFO - tau:0.9801
2024-04-06 20:59:29,756 - train - INFO - True
2024-04-06 20:59:29,757 - train - INFO - alphas:tensor([0.2096, 0.2074, 0.1933, 0.1947, 0.1949], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,787 - train - INFO - tau:0.9801
2024-04-06 20:59:29,787 - train - INFO - True
2024-04-06 20:59:29,787 - train - INFO - alphas:tensor([0.2064, 0.2031, 0.1969, 0.1966, 0.1970], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,817 - train - INFO - tau:0.9801
2024-04-06 20:59:29,817 - train - INFO - True
2024-04-06 20:59:29,818 - train - INFO - alphas:tensor([0.5160, 0.4840], device='cuda:0', grad_fn=<SoftmaxBackward0>)
2024-04-06 20:59:29,818 - train - INFO - tau:0.9801
2024-04-06 20:59:29,818 - train - INFO - avg block size:1.0344827586206897
2024-04-06 20:59:29,818 - train - INFO - current latency ratio:tensor(0.9753)
2024-04-06 20:59:30,006 - train - INFO - Test: [   0/39]  Time: 0.185 (0.185)  Loss:  1.0742 (1.0742)  Acc@1: 71.4844 (71.4844)  Acc@5: 97.6562 (97.6562)
2024-04-06 20:59:31,914 - train - INFO - Test: [  39/39]  Time: 0.042 (0.052)  Loss:  1.1289 (1.0399)  Acc@1: 75.0000 (71.5500)  Acc@5: 100.0000 (97.4700)
2024-04-06 20:59:33,199 - train - INFO - Train: 5 [   0/195 (  0%)]  Loss:  2.068201 (2.0682)  Time: 1.212s,  211.21/s  (1.212s,  211.21/s)  LR: 2.800e-04  Data: 0.213 (0.213)
2024-04-06 21:00:21,893 - train - INFO - Train: 5 [  50/195 ( 26%)]  Loss:  1.670524 (1.9384)  Time: 1.109s,  230.80/s  (0.979s,  261.62/s)  LR: 2.800e-04  Data: 0.014 (0.015)
2024-04-06 21:01:13,817 - train - INFO - Train: 5 [ 100/195 ( 52%)]  Loss:  1.795042 (1.8923)  Time: 1.520s,  168.45/s  (1.008s,  253.92/s)  LR: 2.800e-04  Data: 0.016 (0.012)
