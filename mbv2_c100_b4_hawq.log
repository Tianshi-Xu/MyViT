2024-04-17 15:40:58,934 - train - INFO - aa: rand-m9-mstd0.5-inc1
amp: true
apex_amp: false
aug_splits: 0
batch_size: 256
bn_eps: null
bn_momentum: null
bn_tf: false
budget: 0.125
channels_last: false
checkpoint_hist: 3
clip_grad: null
clip_mode: norm
color_jitter: 0.4
cooldown_epochs: 10
crop_pct: 1.0
cutmix: 1.0
cutmix_minmax: null
data_dir: /home/xts/code/dataset/cifar100/
dataset: torch/cifar100
decay_epochs: 30
decay_rate: 0.1
dist_bn: ''
drop: 0.0
drop_block: null
drop_connect: null
drop_path: null
epoch_repeats: 0.0
epochs: 300
eval_metric: top1
experiment: ''
finetune: false
fix_blocksize: -1
fix_blocksize_list: 1,1,8,1,1,1,16,16,16,16,1,1,16,16,16,16,16,16,1,1,16,16,16,16,1,1,16,16,16,16,1,16
gp: null
hflip: 0.5
img_size: 32
initial_checkpoint: /home/xts/code/njeans/MyViT/pretrained/mbv2_c100.pth.tar
input_size: null
interpolation: bicubic
jsd: false
kd_alpha: 4
lasso_alpha: 0
lasso_beta: 1
local_rank: 0
log_interval: 50
log_name: mbv2_c100_b4_hawq
log_wandb: false
lr: 0.00055
lr_cycle_limit: 1
lr_cycle_mul: 1.0
lr_noise: null
lr_noise_pct: 0.67
lr_noise_std: 1.0
mean:
- 0.5071
- 0.4867
- 0.4408
min_lr: 1.0e-05
mixup: 0.8
mixup_mode: batch
mixup_off_epoch: 175
mixup_prob: 1.0
mixup_switch_prob: 0.5
model: c100_nas_mobilenetv2
model_ema: false
model_ema_decay: 0.9998
model_ema_force_cpu: false
momentum: 0.9
native_amp: false
no_aug: false
no_prefetcher: false
no_resume_opt: false
num_classes: 100
opt: adamw
opt_betas: null
opt_eps: null
output: ''
patience_epochs: 10
pin_mem: false
pretrained: false
ratio:
- 0.75
- 1.3333333333333333
recount: 1
recovery_interval: 0
remode: pixel
reprob: 0.25
resplit: false
resume: ''
save_images: false
scale:
- 0.8
- 1.0
sched: cosine
seed: 3407
smoothing: 0.1
split_bn: false
start_epoch: null
std:
- 0.2675
- 0.2565
- 0.2761
sync_bn: false
tau: 1.0
teacher: c100_mobilenetv2
teacher_checkpoint: /home/xts/code/njeans/MyViT/pretrained/mbv2_c100.pth.tar
torchscript: false
train_interpolation: random
train_split: train
tta: 0
use_kd: true
use_multi_epochs_loader: false
val_split: validation
validation_batch_size_multiplier: 1
vflip: 0.0
warmup_epochs: 0
warmup_lr: 1.0e-05
weight_decay: 0.06
workers: 8

2024-04-17 15:40:58,935 - train - INFO - Training with a single process on 1 GPUs.
2024-04-17 15:40:59,417 - train - INFO - MobileNetV2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (classifier): Linear(in_features=1280, out_features=100, bias=True)
)
2024-04-17 15:41:01,439 - train - INFO - Model c100_nas_mobilenetv2 created, param count:2352163
2024-04-17 15:41:01,453 - train - INFO - Using native Torch AMP. Training in mixed precision.
2024-04-17 15:41:01,453 - train - INFO - Scheduled epochs: 310
2024-04-17 15:41:02,799 - train - INFO - Verifying teacher model
2024-04-17 15:41:03,888 - train - INFO - Test: [   0/39]  Time: 1.088 (1.088)  Loss:  0.9492 (0.9492)  Acc@1: 80.8594 (80.8594)  Acc@5: 95.3125 (95.3125)
2024-04-17 15:41:04,649 - train - INFO - Test: [  39/39]  Time: 0.412 (0.046)  Loss:  1.1045 (0.9844)  Acc@1: 75.0000 (78.7000)  Acc@5: 93.7500 (95.1000)
2024-04-17 15:41:04,649 - train - INFO - Verifying initial model
2024-04-17 15:41:04,785 - train - INFO - Test: [   0/39]  Time: 0.135 (0.135)  Loss:  6.5039 (6.5039)  Acc@1:  1.5625 ( 1.5625)  Acc@5:  3.9062 ( 3.9062)
2024-04-17 15:41:05,807 - train - INFO - Test: [  39/39]  Time: 0.023 (0.029)  Loss:  5.6562 (6.3895)  Acc@1:  0.0000 ( 1.0900)  Acc@5:  6.2500 ( 6.0200)
2024-04-17 15:41:07,381 - train - INFO - Train: 0 [   0/195 (  0%)]  Loss:  4.680233 (4.6802)  Time: 1.570s,  163.01/s  (1.570s,  163.01/s)  LR: 5.500e-04  Data: 0.313 (0.313)
2024-04-17 15:41:12,383 - train - INFO - Train: 0 [  50/195 ( 26%)]  Loss:  3.652316 (4.1323)  Time: 0.095s, 2698.89/s  (0.129s, 1986.79/s)  LR: 5.500e-04  Data: 0.010 (0.015)
2024-04-17 15:41:17,275 - train - INFO - Train: 0 [ 100/195 ( 52%)]  Loss:  3.445738 (3.9436)  Time: 0.092s, 2786.62/s  (0.113s, 2255.81/s)  LR: 5.500e-04  Data: 0.009 (0.012)
2024-04-17 15:41:22,283 - train - INFO - Train: 0 [ 150/195 ( 77%)]  Loss:  3.579559 (3.8153)  Time: 0.101s, 2539.42/s  (0.109s, 2347.32/s)  LR: 5.500e-04  Data: 0.008 (0.011)
2024-04-17 15:41:26,862 - train - INFO - Train: 0 [ 194/195 (100%)]  Loss:  3.817591 (3.7332)  Time: 0.101s, 2532.29/s  (0.108s, 2371.91/s)  LR: 5.500e-04  Data: 0.000 (0.011)
2024-04-17 15:41:26,983 - train - INFO - Test: [   0/39]  Time: 0.118 (0.118)  Loss:  2.0840 (2.0840)  Acc@1: 48.4375 (48.4375)  Acc@5: 76.1719 (76.1719)
2024-04-17 15:41:28,052 - train - INFO - Test: [  39/39]  Time: 0.025 (0.030)  Loss:  1.7373 (2.0504)  Acc@1: 56.2500 (49.1900)  Acc@5: 75.0000 (79.0300)
2024-04-17 15:41:28,341 - train - INFO - Train: 1 [   0/195 (  0%)]  Loss:  3.555800 (3.5558)  Time: 0.215s, 1192.58/s  (0.215s, 1192.58/s)  LR: 5.500e-04  Data: 0.119 (0.119)
2024-04-17 15:41:33,544 - train - INFO - Train: 1 [  50/195 ( 26%)]  Loss:  3.768384 (3.4431)  Time: 0.099s, 2577.93/s  (0.106s, 2410.58/s)  LR: 5.500e-04  Data: 0.011 (0.011)
2024-04-17 15:41:38,812 - train - INFO - Train: 1 [ 100/195 ( 52%)]  Loss:  3.697669 (3.4013)  Time: 0.108s, 2375.92/s  (0.106s, 2420.40/s)  LR: 5.500e-04  Data: 0.010 (0.010)
2024-04-17 15:41:44,060 - train - INFO - Train: 1 [ 150/195 ( 77%)]  Loss:  3.625121 (3.3873)  Time: 0.101s, 2523.17/s  (0.105s, 2426.72/s)  LR: 5.500e-04  Data: 0.007 (0.010)
2024-04-17 15:41:48,641 - train - INFO - Train: 1 [ 194/195 (100%)]  Loss:  3.465256 (3.3571)  Time: 0.093s, 2738.41/s  (0.105s, 2434.03/s)  LR: 5.500e-04  Data: 0.000 (0.010)
2024-04-17 15:41:48,765 - train - INFO - Test: [   0/39]  Time: 0.121 (0.121)  Loss:  1.6465 (1.6465)  Acc@1: 57.4219 (57.4219)  Acc@5: 82.0312 (82.0312)
2024-04-17 15:41:49,780 - train - INFO - Test: [  39/39]  Time: 0.023 (0.028)  Loss:  1.3037 (1.6934)  Acc@1: 81.2500 (56.6800)  Acc@5: 81.2500 (84.4200)
2024-04-17 15:41:50,063 - train - INFO - Train: 2 [   0/195 (  0%)]  Loss:  3.514998 (3.5150)  Time: 0.215s, 1189.39/s  (0.215s, 1189.39/s)  LR: 5.499e-04  Data: 0.134 (0.134)
2024-04-17 15:41:55,331 - train - INFO - Train: 2 [  50/195 ( 26%)]  Loss:  3.308200 (3.1777)  Time: 0.108s, 2379.53/s  (0.107s, 2381.52/s)  LR: 5.499e-04  Data: 0.010 (0.012)
2024-04-17 15:42:00,790 - train - INFO - Train: 2 [ 100/195 ( 52%)]  Loss:  3.194060 (3.1870)  Time: 0.112s, 2286.35/s  (0.108s, 2363.48/s)  LR: 5.499e-04  Data: 0.010 (0.011)
2024-04-17 15:42:06,104 - train - INFO - Train: 2 [ 150/195 ( 77%)]  Loss:  2.823147 (3.1767)  Time: 0.095s, 2681.55/s  (0.108s, 2378.58/s)  LR: 5.499e-04  Data: 0.007 (0.010)
2024-04-17 15:42:10,417 - train - INFO - Train: 2 [ 194/195 (100%)]  Loss:  3.046133 (3.1602)  Time: 0.095s, 2703.95/s  (0.105s, 2427.49/s)  LR: 5.499e-04  Data: 0.000 (0.010)
2024-04-17 15:42:10,537 - train - INFO - Test: [   0/39]  Time: 0.118 (0.118)  Loss:  1.4932 (1.4932)  Acc@1: 61.7188 (61.7188)  Acc@5: 87.8906 (87.8906)
2024-04-17 15:42:11,771 - train - INFO - Test: [  39/39]  Time: 0.023 (0.034)  Loss:  1.0947 (1.5569)  Acc@1: 75.0000 (60.5200)  Acc@5: 87.5000 (86.9700)
2024-04-17 15:42:12,092 - train - INFO - Train: 3 [   0/195 (  0%)]  Loss:  3.431019 (3.4310)  Time: 0.251s, 1019.95/s  (0.251s, 1019.95/s)  LR: 5.499e-04  Data: 0.149 (0.149)
2024-04-17 15:42:16,906 - train - INFO - Train: 3 [  50/195 ( 26%)]  Loss:  3.025009 (3.0800)  Time: 0.095s, 2708.60/s  (0.099s, 2578.27/s)  LR: 5.499e-04  Data: 0.009 (0.011)
2024-04-17 15:42:22,043 - train - INFO - Train: 3 [ 100/195 ( 52%)]  Loss:  2.575159 (3.1102)  Time: 0.122s, 2095.50/s  (0.101s, 2535.03/s)  LR: 5.499e-04  Data: 0.013 (0.010)
2024-04-17 15:42:27,327 - train - INFO - Train: 3 [ 150/195 ( 77%)]  Loss:  3.512478 (3.0998)  Time: 0.111s, 2300.86/s  (0.103s, 2496.84/s)  LR: 5.499e-04  Data: 0.011 (0.010)
2024-04-17 15:42:31,759 - train - INFO - Train: 3 [ 194/195 (100%)]  Loss:  3.538116 (3.1048)  Time: 0.099s, 2593.96/s  (0.102s, 2506.97/s)  LR: 5.499e-04  Data: 0.000 (0.010)
2024-04-17 15:42:31,891 - train - INFO - Test: [   0/39]  Time: 0.130 (0.130)  Loss:  1.4414 (1.4414)  Acc@1: 62.8906 (62.8906)  Acc@5: 87.5000 (87.5000)
2024-04-17 15:42:33,147 - train - INFO - Test: [  39/39]  Time: 0.029 (0.035)  Loss:  1.0889 (1.4734)  Acc@1: 81.2500 (63.0200)  Acc@5: 87.5000 (88.1000)
2024-04-17 15:42:33,471 - train - INFO - Train: 4 [   0/195 (  0%)]  Loss:  3.563118 (3.5631)  Time: 0.242s, 1059.46/s  (0.242s, 1059.46/s)  LR: 5.498e-04  Data: 0.126 (0.126)
2024-04-17 15:42:38,622 - train - INFO - Train: 4 [  50/195 ( 26%)]  Loss:  3.262849 (3.0067)  Time: 0.109s, 2342.88/s  (0.106s, 2421.52/s)  LR: 5.498e-04  Data: 0.011 (0.012)
2024-04-17 15:42:43,733 - train - INFO - Train: 4 [ 100/195 ( 52%)]  Loss:  2.727608 (3.0334)  Time: 0.092s, 2786.18/s  (0.104s, 2462.25/s)  LR: 5.498e-04  Data: 0.009 (0.010)
2024-04-17 15:42:48,784 - train - INFO - Train: 4 [ 150/195 ( 77%)]  Loss:  2.533058 (3.0506)  Time: 0.091s, 2814.36/s  (0.103s, 2485.69/s)  LR: 5.498e-04  Data: 0.010 (0.010)
2024-04-17 15:42:53,268 - train - INFO - Train: 4 [ 194/195 (100%)]  Loss:  2.924282 (3.0625)  Time: 0.102s, 2505.80/s  (0.103s, 2491.77/s)  LR: 5.498e-04  Data: 0.000 (0.010)
2024-04-17 15:42:53,386 - train - INFO - Test: [   0/39]  Time: 0.114 (0.114)  Loss:  1.3252 (1.3252)  Acc@1: 67.5781 (67.5781)  Acc@5: 89.8438 (89.8438)
2024-04-17 15:42:54,404 - train - INFO - Test: [  39/39]  Time: 0.023 (0.028)  Loss:  1.0986 (1.4386)  Acc@1: 81.2500 (64.5200)  Acc@5: 87.5000 (89.1800)
2024-04-17 15:42:54,704 - train - INFO - Train: 5 [   0/195 (  0%)]  Loss:  3.489299 (3.4893)  Time: 0.228s, 1120.40/s  (0.228s, 1120.40/s)  LR: 5.496e-04  Data: 0.140 (0.140)
2024-04-17 15:43:00,021 - train - INFO - Train: 5 [  50/195 ( 26%)]  Loss:  2.791858 (2.9800)  Time: 0.103s, 2481.27/s  (0.109s, 2355.02/s)  LR: 5.496e-04  Data: 0.011 (0.012)
2024-04-17 15:43:05,220 - train - INFO - Train: 5 [ 100/195 ( 52%)]  Loss:  2.449481 (2.9465)  Time: 0.106s, 2409.44/s  (0.106s, 2406.97/s)  LR: 5.496e-04  Data: 0.009 (0.011)
2024-04-17 15:43:10,419 - train - INFO - Train: 5 [ 150/195 ( 77%)]  Loss:  3.229851 (2.9571)  Time: 0.106s, 2407.22/s  (0.106s, 2425.14/s)  LR: 5.496e-04  Data: 0.009 (0.010)
2024-04-17 15:43:14,622 - train - INFO - Train: 5 [ 194/195 (100%)]  Loss:  3.318549 (2.9657)  Time: 0.083s, 3077.43/s  (0.103s, 2478.39/s)  LR: 5.496e-04  Data: 0.000 (0.010)
2024-04-17 15:43:14,747 - train - INFO - Test: [   0/39]  Time: 0.123 (0.123)  Loss:  1.3379 (1.3379)  Acc@1: 67.1875 (67.1875)  Acc@5: 87.8906 (87.8906)
2024-04-17 15:43:15,777 - train - INFO - Test: [  39/39]  Time: 0.023 (0.029)  Loss:  1.1611 (1.3726)  Acc@1: 75.0000 (65.9500)  Acc@5: 87.5000 (90.3200)
2024-04-17 15:43:16,098 - train - INFO - Train: 6 [   0/195 (  0%)]  Loss:  3.305361 (3.3054)  Time: 0.248s, 1032.55/s  (0.248s, 1032.55/s)  LR: 5.495e-04  Data: 0.146 (0.146)
2024-04-17 15:43:21,418 - train - INFO - Train: 6 [  50/195 ( 26%)]  Loss:  3.242266 (3.0128)  Time: 0.101s, 2522.33/s  (0.109s, 2345.67/s)  LR: 5.495e-04  Data: 0.005 (0.012)
2024-04-17 15:43:26,861 - train - INFO - Train: 6 [ 100/195 ( 52%)]  Loss:  3.084560 (3.0021)  Time: 0.112s, 2278.93/s  (0.109s, 2348.74/s)  LR: 5.495e-04  Data: 0.008 (0.011)
