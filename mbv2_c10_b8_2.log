2024-04-14 22:07:24,344 - train - INFO - aa: rand-m9-mstd0.5-inc1
amp: true
apex_amp: false
aug_splits: 0
batch_size: 256
bn_eps: null
bn_momentum: null
bn_tf: false
budget: 0.125
channels_last: false
checkpoint_hist: 3
clip_grad: null
clip_mode: norm
color_jitter: 0.4
cooldown_epochs: 10
crop_pct: 1.0
cutmix: 1.0
cutmix_minmax: null
data_dir: /home/xts/code/dataset/cifar10/
dataset: torch/cifar10
decay_epochs: 30
decay_rate: 0.1
dist_bn: ''
drop: 0.0
drop_block: null
drop_connect: null
drop_path: null
epoch_repeats: 0.0
epochs: 300
eval_metric: top1
experiment: ''
finetune: false
fix_blocksize: -1
fix_blocksize_list: 2,2,2,2,2,2,8,16,8,16,2,2,16,16,16,16,16,16,2,16,16,16,16,16,4,16,16,16,16,16,16,16
gp: null
hflip: 0.5
img_size: 32
initial_checkpoint: /home/xts/code/njeans/MyViT/pretrained/mbv2_c10.pth.tar
input_size: null
interpolation: bicubic
jsd: false
kd_alpha: 4
lasso_alpha: 0
lasso_beta: 1
local_rank: 0
log_interval: 50
log_name: mbv2_c10_b8_2
log_wandb: false
lr: 0.00055
lr_cycle_limit: 1
lr_cycle_mul: 1.0
lr_noise: null
lr_noise_pct: 0.67
lr_noise_std: 1.0
mean:
- 0.4914
- 0.4822
- 0.4465
min_lr: 1.0e-05
mixup: 0.8
mixup_mode: batch
mixup_off_epoch: 175
mixup_prob: 1.0
mixup_switch_prob: 0.5
model: c10_nas_mobilenetv2
model_ema: false
model_ema_decay: 0.9998
model_ema_force_cpu: false
momentum: 0.9
native_amp: false
no_aug: false
no_prefetcher: false
no_resume_opt: false
num_classes: 10
opt: adamw
opt_betas: null
opt_eps: null
output: ''
patience_epochs: 10
pin_mem: false
pretrained: false
ratio:
- 0.75
- 1.3333333333333333
recount: 1
recovery_interval: 0
remode: pixel
reprob: 0.25
resplit: false
resume: ''
save_images: false
scale:
- 0.8
- 1.0
sched: cosine
seed: 3407
smoothing: 0.1
split_bn: false
start_epoch: null
std:
- 0.247
- 0.2435
- 0.2616
sync_bn: false
tau: 1.0
teacher: c10_mobilenetv2
teacher_checkpoint: /home/xts/code/njeans/MyViT/pretrained/mbv2_c10.pth.tar
torchscript: false
train_interpolation: random
train_split: train
tta: 0
use_kd: true
use_multi_epochs_loader: false
val_split: validation
validation_batch_size_multiplier: 1
vflip: 0.0
warmup_epochs: 0
warmup_lr: 1.0e-05
weight_decay: 0.06
workers: 8

2024-04-14 22:07:24,344 - train - INFO - Training with a single process on 1 GPUs.
2024-04-14 22:07:24,771 - train - INFO - MobileNetV2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (classifier): Linear(in_features=1280, out_features=10, bias=True)
)
2024-04-14 22:07:26,763 - train - INFO - Model c10_nas_mobilenetv2 created, param count:2236873
2024-04-14 22:07:26,774 - train - INFO - Using native Torch AMP. Training in mixed precision.
2024-04-14 22:07:26,774 - train - INFO - Scheduled epochs: 310
2024-04-14 22:07:27,885 - train - INFO - Verifying teacher model
2024-04-14 22:07:28,928 - train - INFO - Test: [   0/39]  Time: 1.042 (1.042)  Loss:  0.3928 (0.3928)  Acc@1: 94.1406 (94.1406)  Acc@5: 100.0000 (100.0000)
2024-04-14 22:07:29,598 - train - INFO - Test: [  39/39]  Time: 0.348 (0.043)  Loss:  0.4517 (0.3957)  Acc@1: 93.7500 (94.7100)  Acc@5: 100.0000 (99.8500)
2024-04-14 22:07:29,598 - train - INFO - Verifying initial model
2024-04-14 22:07:29,735 - train - INFO - Test: [   0/39]  Time: 0.136 (0.136)  Loss:  3.3477 (3.3477)  Acc@1:  8.9844 ( 8.9844)  Acc@5: 48.8281 (48.8281)
2024-04-14 22:07:30,862 - train - INFO - Test: [  39/39]  Time: 0.026 (0.032)  Loss:  3.0000 (3.3001)  Acc@1: 18.7500 (10.0000)  Acc@5: 56.2500 (50.0000)
2024-04-14 22:07:32,479 - train - INFO - Train: 0 [   0/195 (  0%)]  Loss:  2.464159 (2.4642)  Time: 1.614s,  158.61/s  (1.614s,  158.61/s)  LR: 5.500e-04  Data: 0.302 (0.302)
2024-04-14 22:07:38,011 - train - INFO - Train: 0 [  50/195 ( 26%)]  Loss:  2.040475 (2.1758)  Time: 0.112s, 2285.33/s  (0.140s, 1827.71/s)  LR: 5.500e-04  Data: 0.010 (0.015)
2024-04-14 22:07:43,551 - train - INFO - Train: 0 [ 100/195 ( 52%)]  Loss:  2.166728 (2.1228)  Time: 0.121s, 2107.34/s  (0.126s, 2039.10/s)  LR: 5.500e-04  Data: 0.011 (0.013)
2024-04-14 22:07:49,204 - train - INFO - Train: 0 [ 150/195 ( 77%)]  Loss:  2.082531 (2.0870)  Time: 0.111s, 2307.38/s  (0.121s, 2108.79/s)  LR: 5.500e-04  Data: 0.010 (0.012)
2024-04-14 22:07:54,216 - train - INFO - Train: 0 [ 194/195 (100%)]  Loss:  1.945867 (2.0605)  Time: 0.104s, 2465.89/s  (0.120s, 2138.77/s)  LR: 5.500e-04  Data: 0.000 (0.011)
2024-04-14 22:07:54,349 - train - INFO - Test: [   0/39]  Time: 0.128 (0.128)  Loss:  1.3467 (1.3467)  Acc@1: 56.2500 (56.2500)  Acc@5: 95.3125 (95.3125)
2024-04-14 22:07:55,528 - train - INFO - Test: [  39/39]  Time: 0.027 (0.033)  Loss:  1.4307 (1.3572)  Acc@1: 43.7500 (54.0200)  Acc@5: 100.0000 (95.1700)
2024-04-14 22:07:55,841 - train - INFO - Train: 1 [   0/195 (  0%)]  Loss:  1.964738 (1.9647)  Time: 0.242s, 1056.37/s  (0.242s, 1056.37/s)  LR: 5.500e-04  Data: 0.146 (0.146)
2024-04-14 22:08:01,329 - train - INFO - Train: 1 [  50/195 ( 26%)]  Loss:  1.879240 (1.9231)  Time: 0.107s, 2392.49/s  (0.112s, 2279.05/s)  LR: 5.500e-04  Data: 0.011 (0.012)
2024-04-14 22:08:06,878 - train - INFO - Train: 1 [ 100/195 ( 52%)]  Loss:  2.045659 (1.9198)  Time: 0.111s, 2296.75/s  (0.112s, 2292.88/s)  LR: 5.500e-04  Data: 0.011 (0.011)
2024-04-14 22:08:12,455 - train - INFO - Train: 1 [ 150/195 ( 77%)]  Loss:  1.871979 (1.9029)  Time: 0.115s, 2220.60/s  (0.112s, 2293.96/s)  LR: 5.500e-04  Data: 0.011 (0.011)
2024-04-14 22:08:17,596 - train - INFO - Train: 1 [ 194/195 (100%)]  Loss:  1.749847 (1.8923)  Time: 0.103s, 2489.85/s  (0.113s, 2269.95/s)  LR: 5.500e-04  Data: 0.000 (0.011)
2024-04-14 22:08:17,723 - train - INFO - Test: [   0/39]  Time: 0.123 (0.123)  Loss:  0.9956 (0.9956)  Acc@1: 69.1406 (69.1406)  Acc@5: 98.0469 (98.0469)
2024-04-14 22:08:18,930 - train - INFO - Test: [  39/39]  Time: 0.027 (0.033)  Loss:  0.9468 (1.0004)  Acc@1: 75.0000 (69.6400)  Acc@5: 100.0000 (97.2100)
2024-04-14 22:08:19,234 - train - INFO - Train: 2 [   0/195 (  0%)]  Loss:  1.757673 (1.7577)  Time: 0.236s, 1086.22/s  (0.236s, 1086.22/s)  LR: 5.499e-04  Data: 0.139 (0.139)
2024-04-14 22:08:25,027 - train - INFO - Train: 2 [  50/195 ( 26%)]  Loss:  1.883772 (1.8552)  Time: 0.117s, 2181.65/s  (0.118s, 2165.98/s)  LR: 5.499e-04  Data: 0.010 (0.012)
2024-04-14 22:08:30,785 - train - INFO - Train: 2 [ 100/195 ( 52%)]  Loss:  1.682623 (1.8496)  Time: 0.097s, 2630.62/s  (0.117s, 2194.22/s)  LR: 5.499e-04  Data: 0.007 (0.011)
2024-04-14 22:08:36,468 - train - INFO - Train: 2 [ 150/195 ( 77%)]  Loss:  1.680087 (1.8339)  Time: 0.110s, 2325.33/s  (0.116s, 2213.45/s)  LR: 5.499e-04  Data: 0.010 (0.010)
2024-04-14 22:08:41,358 - train - INFO - Train: 2 [ 194/195 (100%)]  Loss:  1.923306 (1.8269)  Time: 0.104s, 2460.09/s  (0.115s, 2233.24/s)  LR: 5.499e-04  Data: 0.000 (0.010)
2024-04-14 22:08:41,476 - train - INFO - Test: [   0/39]  Time: 0.115 (0.115)  Loss:  0.9707 (0.9707)  Acc@1: 71.8750 (71.8750)  Acc@5: 98.0469 (98.0469)
2024-04-14 22:08:42,676 - train - INFO - Test: [  39/39]  Time: 0.028 (0.033)  Loss:  0.9644 (0.9982)  Acc@1: 68.7500 (68.4900)  Acc@5: 100.0000 (97.4700)
2024-04-14 22:08:42,996 - train - INFO - Train: 3 [   0/195 (  0%)]  Loss:  1.832278 (1.8323)  Time: 0.254s, 1006.67/s  (0.254s, 1006.67/s)  LR: 5.499e-04  Data: 0.146 (0.146)
2024-04-14 22:08:48,622 - train - INFO - Train: 3 [  50/195 ( 26%)]  Loss:  1.704055 (1.7896)  Time: 0.122s, 2099.73/s  (0.115s, 2220.89/s)  LR: 5.499e-04  Data: 0.010 (0.012)
2024-04-14 22:08:54,134 - train - INFO - Train: 3 [ 100/195 ( 52%)]  Loss:  1.731610 (1.7794)  Time: 0.107s, 2383.72/s  (0.113s, 2270.25/s)  LR: 5.499e-04  Data: 0.011 (0.011)
2024-04-14 22:08:59,515 - train - INFO - Train: 3 [ 150/195 ( 77%)]  Loss:  1.918848 (1.7850)  Time: 0.099s, 2574.62/s  (0.111s, 2305.36/s)  LR: 5.499e-04  Data: 0.006 (0.010)
2024-04-14 22:09:04,270 - train - INFO - Train: 3 [ 194/195 (100%)]  Loss:  1.753225 (1.7823)  Time: 0.094s, 2720.95/s  (0.110s, 2319.50/s)  LR: 5.499e-04  Data: 0.000 (0.010)
2024-04-14 22:09:04,396 - train - INFO - Test: [   0/39]  Time: 0.124 (0.124)  Loss:  0.9126 (0.9126)  Acc@1: 75.7812 (75.7812)  Acc@5: 98.0469 (98.0469)
2024-04-14 22:09:05,605 - train - INFO - Test: [  39/39]  Time: 0.028 (0.033)  Loss:  0.8950 (0.9138)  Acc@1: 75.0000 (73.5700)  Acc@5: 100.0000 (97.9000)
2024-04-14 22:09:05,937 - train - INFO - Train: 4 [   0/195 (  0%)]  Loss:  1.657449 (1.6574)  Time: 0.256s, 1001.02/s  (0.256s, 1001.02/s)  LR: 5.498e-04  Data: 0.146 (0.146)
2024-04-14 22:09:11,402 - train - INFO - Train: 4 [  50/195 ( 26%)]  Loss:  1.630371 (1.7719)  Time: 0.117s, 2190.65/s  (0.112s, 2282.72/s)  LR: 5.498e-04  Data: 0.010 (0.012)
2024-04-14 22:09:16,965 - train - INFO - Train: 4 [ 100/195 ( 52%)]  Loss:  1.740416 (1.7680)  Time: 0.115s, 2233.86/s  (0.112s, 2291.97/s)  LR: 5.498e-04  Data: 0.010 (0.011)
2024-04-14 22:09:22,719 - train - INFO - Train: 4 [ 150/195 ( 77%)]  Loss:  1.680874 (1.7602)  Time: 0.107s, 2393.20/s  (0.113s, 2269.48/s)  LR: 5.498e-04  Data: 0.011 (0.011)
2024-04-14 22:09:27,744 - train - INFO - Train: 4 [ 194/195 (100%)]  Loss:  1.602906 (1.7542)  Time: 0.100s, 2561.13/s  (0.113s, 2263.28/s)  LR: 5.498e-04  Data: 0.000 (0.010)
2024-04-14 22:09:27,848 - train - INFO - Test: [   0/39]  Time: 0.103 (0.103)  Loss:  0.8564 (0.8564)  Acc@1: 75.7812 (75.7812)  Acc@5: 98.0469 (98.0469)
2024-04-14 22:09:29,109 - train - INFO - Test: [  39/39]  Time: 0.028 (0.034)  Loss:  0.6582 (0.8462)  Acc@1: 87.5000 (75.9300)  Acc@5: 100.0000 (98.3900)
2024-04-14 22:09:29,404 - train - INFO - Train: 5 [   0/195 (  0%)]  Loss:  1.838870 (1.8389)  Time: 0.222s, 1154.52/s  (0.222s, 1154.52/s)  LR: 5.496e-04  Data: 0.132 (0.132)
2024-04-14 22:09:35,129 - train - INFO - Train: 5 [  50/195 ( 26%)]  Loss:  1.789533 (1.7420)  Time: 0.118s, 2171.74/s  (0.117s, 2195.90/s)  LR: 5.496e-04  Data: 0.008 (0.011)
2024-04-14 22:09:41,058 - train - INFO - Train: 5 [ 100/195 ( 52%)]  Loss:  1.905557 (1.7282)  Time: 0.104s, 2455.62/s  (0.118s, 2177.69/s)  LR: 5.496e-04  Data: 0.010 (0.011)
2024-04-14 22:09:46,677 - train - INFO - Train: 5 [ 150/195 ( 77%)]  Loss:  1.612153 (1.7259)  Time: 0.106s, 2407.09/s  (0.116s, 2209.98/s)  LR: 5.496e-04  Data: 0.010 (0.010)
2024-04-14 22:09:51,536 - train - INFO - Train: 5 [ 194/195 (100%)]  Loss:  1.571307 (1.7278)  Time: 0.115s, 2223.68/s  (0.115s, 2233.70/s)  LR: 5.496e-04  Data: 0.000 (0.010)
2024-04-14 22:09:51,655 - train - INFO - Test: [   0/39]  Time: 0.117 (0.117)  Loss:  0.8433 (0.8433)  Acc@1: 75.3906 (75.3906)  Acc@5: 97.6562 (97.6562)
2024-04-14 22:09:52,857 - train - INFO - Test: [  39/39]  Time: 0.027 (0.033)  Loss:  0.7437 (0.8488)  Acc@1: 81.2500 (75.4700)  Acc@5: 100.0000 (97.9000)
2024-04-14 22:09:53,167 - train - INFO - Train: 6 [   0/195 (  0%)]  Loss:  1.504309 (1.5043)  Time: 0.239s, 1068.91/s  (0.239s, 1068.91/s)  LR: 5.495e-04  Data: 0.136 (0.136)
2024-04-14 22:09:58,651 - train - INFO - Train: 6 [  50/195 ( 26%)]  Loss:  1.774038 (1.7431)  Time: 0.111s, 2302.86/s  (0.112s, 2281.75/s)  LR: 5.495e-04  Data: 0.011 (0.012)
2024-04-14 22:10:04,072 - train - INFO - Train: 6 [ 100/195 ( 52%)]  Loss:  1.551302 (1.7363)  Time: 0.103s, 2473.61/s  (0.110s, 2320.66/s)  LR: 5.495e-04  Data: 0.010 (0.011)
2024-04-14 22:10:09,653 - train - INFO - Train: 6 [ 150/195 ( 77%)]  Loss:  1.585186 (1.7309)  Time: 0.115s, 2217.09/s  (0.111s, 2311.76/s)  LR: 5.495e-04  Data: 0.008 (0.011)
2024-04-14 22:10:14,821 - train - INFO - Train: 6 [ 194/195 (100%)]  Loss:  1.886858 (1.7316)  Time: 0.109s, 2346.44/s  (0.112s, 2280.67/s)  LR: 5.495e-04  Data: 0.000 (0.010)
2024-04-14 22:10:14,963 - train - INFO - Test: [   0/39]  Time: 0.139 (0.139)  Loss:  0.8413 (0.8413)  Acc@1: 80.8594 (80.8594)  Acc@5: 98.4375 (98.4375)
2024-04-14 22:10:16,209 - train - INFO - Test: [  39/39]  Time: 0.030 (0.035)  Loss:  0.6943 (0.8411)  Acc@1: 81.2500 (79.0600)  Acc@5: 100.0000 (98.6200)
2024-04-14 22:10:16,495 - train - INFO - Train: 7 [   0/195 (  0%)]  Loss:  1.628038 (1.6280)  Time: 0.211s, 1212.74/s  (0.211s, 1212.74/s)  LR: 5.493e-04  Data: 0.123 (0.123)
2024-04-14 22:10:21,675 - train - INFO - Train: 7 [  50/195 ( 26%)]  Loss:  1.583198 (1.7196)  Time: 0.106s, 2414.56/s  (0.106s, 2422.31/s)  LR: 5.493e-04  Data: 0.010 (0.009)
2024-04-14 22:10:27,201 - train - INFO - Train: 7 [ 100/195 ( 52%)]  Loss:  1.561662 (1.6929)  Time: 0.107s, 2383.84/s  (0.108s, 2369.10/s)  LR: 5.493e-04  Data: 0.011 (0.009)
2024-04-14 22:10:32,519 - train - INFO - Train: 7 [ 150/195 ( 77%)]  Loss:  1.778006 (1.6978)  Time: 0.107s, 2396.42/s  (0.107s, 2381.73/s)  LR: 5.493e-04  Data: 0.011 (0.009)
2024-04-14 22:10:37,167 - train - INFO - Train: 7 [ 194/195 (100%)]  Loss:  1.661648 (1.6950)  Time: 0.098s, 2624.91/s  (0.107s, 2391.14/s)  LR: 5.493e-04  Data: 0.000 (0.010)
2024-04-14 22:10:37,291 - train - INFO - Test: [   0/39]  Time: 0.121 (0.121)  Loss:  0.8242 (0.8242)  Acc@1: 79.6875 (79.6875)  Acc@5: 98.0469 (98.0469)
2024-04-14 22:10:38,448 - train - INFO - Test: [  39/39]  Time: 0.027 (0.032)  Loss:  0.6685 (0.8049)  Acc@1: 81.2500 (79.5100)  Acc@5: 100.0000 (98.5500)
2024-04-14 22:10:38,762 - train - INFO - Train: 8 [   0/195 (  0%)]  Loss:  1.911015 (1.9110)  Time: 0.245s, 1044.37/s  (0.245s, 1044.37/s)  LR: 5.491e-04  Data: 0.136 (0.136)
2024-04-14 22:10:44,451 - train - INFO - Train: 8 [  50/195 ( 26%)]  Loss:  1.719822 (1.6809)  Time: 0.106s, 2411.89/s  (0.116s, 2200.51/s)  LR: 5.491e-04  Data: 0.011 (0.012)
2024-04-14 22:10:50,000 - train - INFO - Train: 8 [ 100/195 ( 52%)]  Loss:  1.852447 (1.6832)  Time: 0.107s, 2385.31/s  (0.114s, 2252.22/s)  LR: 5.491e-04  Data: 0.012 (0.011)
2024-04-14 22:10:55,487 - train - INFO - Train: 8 [ 150/195 ( 77%)]  Loss:  1.763133 (1.6700)  Time: 0.101s, 2543.36/s  (0.112s, 2278.47/s)  LR: 5.491e-04  Data: 0.008 (0.011)
2024-04-14 22:10:59,775 - train - INFO - Train: 8 [ 194/195 (100%)]  Loss:  1.552846 (1.6682)  Time: 0.088s, 2908.69/s  (0.109s, 2348.86/s)  LR: 5.491e-04  Data: 0.000 (0.010)
2024-04-14 22:10:59,878 - train - INFO - Test: [   0/39]  Time: 0.101 (0.101)  Loss:  0.8032 (0.8032)  Acc@1: 78.5156 (78.5156)  Acc@5: 99.2188 (99.2188)
2024-04-14 22:11:01,187 - train - INFO - Test: [  39/39]  Time: 0.030 (0.035)  Loss:  0.6812 (0.7836)  Acc@1: 81.2500 (80.1700)  Acc@5: 100.0000 (98.7500)
2024-04-14 22:11:01,493 - train - INFO - Train: 9 [   0/195 (  0%)]  Loss:  1.893935 (1.8939)  Time: 0.223s, 1149.22/s  (0.223s, 1149.22/s)  LR: 5.488e-04  Data: 0.130 (0.130)
2024-04-14 22:11:06,379 - train - INFO - Train: 9 [  50/195 ( 26%)]  Loss:  1.705302 (1.6904)  Time: 0.094s, 2735.57/s  (0.100s, 2556.24/s)  LR: 5.488e-04  Data: 0.005 (0.008)
2024-04-14 22:11:11,176 - train - INFO - Train: 9 [ 100/195 ( 52%)]  Loss:  1.830457 (1.6962)  Time: 0.092s, 2775.15/s  (0.098s, 2610.70/s)  LR: 5.488e-04  Data: 0.005 (0.007)
2024-04-14 22:11:15,994 - train - INFO - Train: 9 [ 150/195 ( 77%)]  Loss:  1.536524 (1.6821)  Time: 0.096s, 2680.41/s  (0.097s, 2625.95/s)  LR: 5.488e-04  Data: 0.005 (0.007)
