2024-04-27 21:59:44,922 - train - INFO - aa: rand-m9-mstd0.5-inc1
amp: true
apex_amp: false
aug_splits: 0
batch_size: 256
bn_eps: null
bn_momentum: null
bn_tf: false
channels_last: false
checkpoint_hist: 3
clip_grad: null
clip_mode: norm
color_jitter: 0.4
cooldown_epochs: 10
crop_pct: 1.0
cutmix: 1.0
cutmix_minmax: null
data_dir: /home/xts/code/dataset/cifar10/
dataset: torch/cifar10
decay_epochs: 30
decay_rate: 0.1
dist_bn: ''
drop: 0.0
drop_block: null
drop_connect: null
drop_path: null
epoch_repeats: 0.0
epochs: 300
eval_metric: top1
experiment: ''
gp: null
hflip: 0.5
img_size: 32
initial_checkpoint: /home/xts/code/njeans/MyViT/pretrained/mbv2_c10.pth.tar
input_size: null
interpolation: bicubic
jsd: false
kd_alpha: 4
local_rank: 0
log_interval: 50
log_name: mbv2_c10_prune_b8
log_wandb: false
lr: 0.00055
lr_cycle_limit: 1
lr_cycle_mul: 1.0
lr_noise: null
lr_noise_pct: 0.67
lr_noise_std: 1.0
mean:
- 0.4914
- 0.4822
- 0.4465
min_lr: 1.0e-05
mixup: 0.8
mixup_mode: batch
mixup_off_epoch: 175
mixup_prob: 1.0
mixup_switch_prob: 0.5
model: c10_prune_mobilenetv2
model_ema: false
model_ema_decay: 0.9998
model_ema_force_cpu: false
momentum: 0.9
native_amp: false
no_aug: false
no_prefetcher: false
no_resume_opt: false
num_classes: 10
opt: adamw
opt_betas: null
opt_eps: null
output: ''
patience_epochs: 10
pin_mem: false
pretrained: false
prune_ratio: 88
ratio:
- 0.75
- 1.3333333333333333
recount: 1
recovery_interval: 0
remode: pixel
reprob: 0.25
resplit: false
resume: ''
save_images: false
scale:
- 0.8
- 1.0
sched: cosine
seed: 3407
smoothing: 0.1
split_bn: false
start_epoch: null
std:
- 0.247
- 0.2435
- 0.2616
sync_bn: false
teacher: c10_mobilenetv2
teacher_checkpoint: /home/xts/code/njeans/MyViT/pretrained/mbv2_c10.pth.tar
torchscript: false
train_interpolation: random
train_split: train
tta: 0
use_kd: true
use_multi_epochs_loader: false
val_split: validation
validation_batch_size_multiplier: 1
vflip: 0.0
warmup_epochs: 10
warmup_lr: 1.0e-05
weight_decay: 0.06
workers: 4

2024-04-27 21:59:44,922 - train - INFO - Training with a single process on 1 GPUs.
2024-04-27 21:59:46,718 - train - INFO - MobileNetV2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (classifier): Linear(in_features=1280, out_features=10, bias=True)
)
2024-04-27 21:59:47,288 - train - INFO - Model c10_prune_mobilenetv2 created, param count:2236682
2024-04-27 21:59:47,300 - train - INFO - Using native Torch AMP. Training in mixed precision.
2024-04-27 21:59:47,301 - train - INFO - Scheduled epochs: 310
2024-04-27 21:59:48,968 - train - INFO - Verifying teacher model
2024-04-27 21:59:51,450 - train - INFO - Test: [   0/39]  Time: 2.481 (2.481)  Loss:  0.3931 (0.3931)  Acc@1: 94.1406 (94.1406)  Acc@5: 100.0000 (100.0000)
2024-04-27 21:59:52,717 - train - INFO - Test: [  39/39]  Time: 0.568 (0.094)  Loss:  0.4517 (0.3957)  Acc@1: 93.7500 (94.7100)  Acc@5: 100.0000 (99.8500)
2024-04-27 21:59:52,718 - train - INFO - Verifying initial model
2024-04-27 21:59:52,863 - train - INFO - Test: [   0/39]  Time: 0.144 (0.144)  Loss:  0.3931 (0.3931)  Acc@1: 94.1406 (94.1406)  Acc@5: 100.0000 (100.0000)
2024-04-27 21:59:53,586 - train - INFO - Test: [  39/39]  Time: 0.011 (0.022)  Loss:  0.4517 (0.3957)  Acc@1: 93.7500 (94.7100)  Acc@5: 100.0000 (99.8500)
2024-04-27 22:00:00,262 - train - INFO - Total Mul: 2035.888888888889, Total Rot: 1017.7
2024-04-27 22:00:02,708 - train - INFO - Train: 0 [   0/195 (  0%)]  Loss:  2.491541 (2.4915)  Time: 2.441s,  104.86/s  (2.441s,  104.86/s)  LR: 1.000e-05  Data: 0.350 (0.350)
2024-04-27 22:00:07,988 - train - INFO - Train: 0 [  50/195 ( 26%)]  Loss:  2.454944 (2.4528)  Time: 0.119s, 2147.58/s  (0.151s, 1691.26/s)  LR: 1.000e-05  Data: 0.007 (0.014)
2024-04-27 22:00:15,804 - train - INFO - Train: 0 [ 100/195 ( 52%)]  Loss:  2.415595 (2.4297)  Time: 0.165s, 1554.60/s  (0.154s, 1664.42/s)  LR: 1.000e-05  Data: 0.006 (0.011)
2024-04-27 22:00:23,735 - train - INFO - Train: 0 [ 150/195 ( 77%)]  Loss:  2.369407 (2.4131)  Time: 0.163s, 1572.16/s  (0.155s, 1647.45/s)  LR: 1.000e-05  Data: 0.006 (0.009)
2024-04-27 22:00:30,873 - train - INFO - Train: 0 [ 194/195 (100%)]  Loss:  2.309856 (2.3973)  Time: 0.157s, 1634.51/s  (0.157s, 1631.30/s)  LR: 1.000e-05  Data: 0.000 (0.009)
2024-04-27 22:00:31,062 - train - INFO - Test: [   0/39]  Time: 0.187 (0.187)  Loss:  2.2109 (2.2109)  Acc@1: 16.4062 (16.4062)  Acc@5: 67.5781 (67.5781)
2024-04-27 22:00:32,689 - train - INFO - Test: [  39/39]  Time: 0.010 (0.045)  Loss:  2.2676 (2.1994)  Acc@1:  6.2500 (16.6800)  Acc@5: 62.5000 (68.1400)
2024-04-27 22:00:33,087 - train - INFO - Train: 1 [   0/195 (  0%)]  Loss:  2.355155 (2.3552)  Time: 0.330s,  775.34/s  (0.330s,  775.34/s)  LR: 6.400e-05  Data: 0.219 (0.219)
2024-04-27 22:00:41,209 - train - INFO - Train: 1 [  50/195 ( 26%)]  Loss:  2.302504 (2.3027)  Time: 0.157s, 1627.96/s  (0.166s, 1544.92/s)  LR: 6.400e-05  Data: 0.010 (0.010)
2024-04-27 22:00:48,523 - train - INFO - Train: 1 [ 100/195 ( 52%)]  Loss:  2.246906 (2.2795)  Time: 0.165s, 1547.12/s  (0.156s, 1640.20/s)  LR: 6.400e-05  Data: 0.007 (0.009)
2024-04-27 22:00:56,692 - train - INFO - Train: 1 [ 150/195 ( 77%)]  Loss:  2.186626 (2.2630)  Time: 0.162s, 1584.43/s  (0.158s, 1615.23/s)  LR: 6.400e-05  Data: 0.006 (0.008)
2024-04-27 22:01:03,898 - train - INFO - Train: 1 [ 194/195 (100%)]  Loss:  2.287081 (2.2521)  Time: 0.161s, 1587.62/s  (0.160s, 1603.20/s)  LR: 6.400e-05  Data: 0.000 (0.008)
2024-04-27 22:01:04,083 - train - INFO - Test: [   0/39]  Time: 0.183 (0.183)  Loss:  1.9004 (1.9004)  Acc@1: 33.5938 (33.5938)  Acc@5: 82.8125 (82.8125)
2024-04-27 22:01:05,746 - train - INFO - Test: [  39/39]  Time: 0.012 (0.046)  Loss:  1.8574 (1.9016)  Acc@1: 31.2500 (31.4900)  Acc@5: 87.5000 (84.2100)
2024-04-27 22:01:06,156 - train - INFO - Train: 2 [   0/195 (  0%)]  Loss:  2.162627 (2.1626)  Time: 0.333s,  769.01/s  (0.333s,  769.01/s)  LR: 1.180e-04  Data: 0.180 (0.180)
2024-04-27 22:01:14,376 - train - INFO - Train: 2 [  50/195 ( 26%)]  Loss:  2.242454 (2.2084)  Time: 0.163s, 1566.08/s  (0.168s, 1526.55/s)  LR: 1.180e-04  Data: 0.005 (0.010)
2024-04-27 22:01:21,592 - train - INFO - Train: 2 [ 100/195 ( 52%)]  Loss:  2.230504 (2.1935)  Time: 0.168s, 1522.34/s  (0.156s, 1639.82/s)  LR: 1.180e-04  Data: 0.007 (0.008)
2024-04-27 22:01:29,818 - train - INFO - Train: 2 [ 150/195 ( 77%)]  Loss:  2.185255 (2.1824)  Time: 0.164s, 1557.67/s  (0.159s, 1611.21/s)  LR: 1.180e-04  Data: 0.005 (0.007)
2024-04-27 22:01:36,989 - train - INFO - Train: 2 [ 194/195 (100%)]  Loss:  2.210921 (2.1775)  Time: 0.166s, 1544.10/s  (0.160s, 1601.90/s)  LR: 1.180e-04  Data: 0.000 (0.007)
2024-04-27 22:01:37,155 - train - INFO - Test: [   0/39]  Time: 0.164 (0.164)  Loss:  1.7383 (1.7383)  Acc@1: 39.4531 (39.4531)  Acc@5: 88.2812 (88.2812)
2024-04-27 22:01:38,768 - train - INFO - Test: [  39/39]  Time: 0.045 (0.044)  Loss:  1.7588 (1.7377)  Acc@1: 31.2500 (39.4800)  Acc@5: 87.5000 (89.0000)
2024-04-27 22:01:39,179 - train - INFO - Train: 3 [   0/195 (  0%)]  Loss:  2.178525 (2.1785)  Time: 0.335s,  763.33/s  (0.335s,  763.33/s)  LR: 1.720e-04  Data: 0.177 (0.177)
2024-04-27 22:01:47,377 - train - INFO - Train: 3 [  50/195 ( 26%)]  Loss:  2.107561 (2.1648)  Time: 0.162s, 1577.52/s  (0.167s, 1530.14/s)  LR: 1.720e-04  Data: 0.005 (0.009)
2024-04-27 22:01:54,326 - train - INFO - Train: 3 [ 100/195 ( 52%)]  Loss:  2.111314 (2.1407)  Time: 0.165s, 1552.74/s  (0.153s, 1670.25/s)  LR: 1.720e-04  Data: 0.005 (0.007)
2024-04-27 22:02:02,497 - train - INFO - Train: 3 [ 150/195 ( 77%)]  Loss:  2.210614 (2.1358)  Time: 0.168s, 1522.51/s  (0.157s, 1634.47/s)  LR: 1.720e-04  Data: 0.006 (0.007)
2024-04-27 22:02:09,786 - train - INFO - Train: 3 [ 194/195 (100%)]  Loss:  2.186584 (2.1300)  Time: 0.158s, 1625.08/s  (0.159s, 1613.54/s)  LR: 1.720e-04  Data: 0.000 (0.007)
2024-04-27 22:02:09,963 - train - INFO - Test: [   0/39]  Time: 0.176 (0.176)  Loss:  1.6094 (1.6094)  Acc@1: 44.9219 (44.9219)  Acc@5: 92.9688 (92.9688)
2024-04-27 22:02:11,571 - train - INFO - Test: [  39/39]  Time: 0.010 (0.045)  Loss:  1.7676 (1.6082)  Acc@1: 25.0000 (43.9700)  Acc@5: 81.2500 (91.2200)
2024-04-27 22:02:11,971 - train - INFO - Train: 4 [   0/195 (  0%)]  Loss:  2.138645 (2.1386)  Time: 0.324s,  790.84/s  (0.324s,  790.84/s)  LR: 2.260e-04  Data: 0.185 (0.185)
2024-04-27 22:02:20,273 - train - INFO - Train: 4 [  50/195 ( 26%)]  Loss:  2.028868 (2.0929)  Time: 0.166s, 1542.63/s  (0.169s, 1513.80/s)  LR: 2.260e-04  Data: 0.004 (0.010)
2024-04-27 22:02:27,656 - train - INFO - Train: 4 [ 100/195 ( 52%)]  Loss:  2.132511 (2.0801)  Time: 0.164s, 1558.69/s  (0.158s, 1615.31/s)  LR: 2.260e-04  Data: 0.005 (0.009)
2024-04-27 22:02:35,995 - train - INFO - Train: 4 [ 150/195 ( 77%)]  Loss:  2.134480 (2.0766)  Time: 0.166s, 1537.96/s  (0.161s, 1587.88/s)  LR: 2.260e-04  Data: 0.005 (0.008)
2024-04-27 22:02:43,340 - train - INFO - Train: 4 [ 194/195 (100%)]  Loss:  2.059224 (2.0720)  Time: 0.167s, 1533.35/s  (0.163s, 1575.35/s)  LR: 2.260e-04  Data: 0.000 (0.008)
2024-04-27 22:02:43,518 - train - INFO - Test: [   0/39]  Time: 0.175 (0.175)  Loss:  1.4990 (1.4990)  Acc@1: 48.4375 (48.4375)  Acc@5: 92.9688 (92.9688)
2024-04-27 22:02:45,207 - train - INFO - Test: [  39/39]  Time: 0.011 (0.047)  Loss:  1.5547 (1.4933)  Acc@1: 43.7500 (49.2200)  Acc@5: 87.5000 (92.6300)
2024-04-27 22:02:45,609 - train - INFO - Train: 5 [   0/195 (  0%)]  Loss:  2.083136 (2.0831)  Time: 0.323s,  791.49/s  (0.323s,  791.49/s)  LR: 2.800e-04  Data: 0.168 (0.168)
2024-04-27 22:02:53,959 - train - INFO - Train: 5 [  50/195 ( 26%)]  Loss:  2.054544 (2.0648)  Time: 0.163s, 1568.18/s  (0.170s, 1505.51/s)  LR: 2.800e-04  Data: 0.005 (0.011)
2024-04-27 22:03:01,362 - train - INFO - Train: 5 [ 100/195 ( 52%)]  Loss:  2.076978 (2.0523)  Time: 0.167s, 1534.41/s  (0.159s, 1608.54/s)  LR: 2.800e-04  Data: 0.006 (0.009)
2024-04-27 22:03:09,695 - train - INFO - Train: 5 [ 150/195 ( 77%)]  Loss:  1.944097 (2.0387)  Time: 0.168s, 1522.76/s  (0.162s, 1583.91/s)  LR: 2.800e-04  Data: 0.005 (0.008)
2024-04-27 22:03:17,047 - train - INFO - Train: 5 [ 194/195 (100%)]  Loss:  2.143759 (2.0358)  Time: 0.163s, 1571.29/s  (0.163s, 1571.98/s)  LR: 2.800e-04  Data: 0.000 (0.008)
2024-04-27 22:03:17,235 - train - INFO - Test: [   0/39]  Time: 0.187 (0.187)  Loss:  1.3926 (1.3926)  Acc@1: 50.3906 (50.3906)  Acc@5: 94.9219 (94.9219)
2024-04-27 22:03:18,831 - train - INFO - Test: [  39/39]  Time: 0.057 (0.045)  Loss:  1.4268 (1.3834)  Acc@1: 50.0000 (53.9500)  Acc@5: 87.5000 (94.2400)
2024-04-27 22:03:19,245 - train - INFO - Train: 6 [   0/195 (  0%)]  Loss:  2.014689 (2.0147)  Time: 0.336s,  762.95/s  (0.336s,  762.95/s)  LR: 3.340e-04  Data: 0.171 (0.171)
2024-04-27 22:03:27,606 - train - INFO - Train: 6 [  50/195 ( 26%)]  Loss:  2.109643 (2.0235)  Time: 0.161s, 1588.61/s  (0.171s, 1501.44/s)  LR: 3.340e-04  Data: 0.009 (0.011)
2024-04-27 22:03:35,042 - train - INFO - Train: 6 [ 100/195 ( 52%)]  Loss:  2.053061 (2.0166)  Time: 0.165s, 1547.03/s  (0.160s, 1602.91/s)  LR: 3.340e-04  Data: 0.007 (0.009)
2024-04-27 22:03:43,312 - train - INFO - Train: 6 [ 150/195 ( 77%)]  Loss:  2.018810 (2.0138)  Time: 0.166s, 1539.52/s  (0.162s, 1584.25/s)  LR: 3.340e-04  Data: 0.005 (0.008)
2024-04-27 22:03:50,640 - train - INFO - Train: 6 [ 194/195 (100%)]  Loss:  2.016809 (2.0127)  Time: 0.162s, 1579.94/s  (0.163s, 1573.40/s)  LR: 3.340e-04  Data: 0.000 (0.007)
2024-04-27 22:03:50,834 - train - INFO - Test: [   0/39]  Time: 0.192 (0.192)  Loss:  1.3105 (1.3105)  Acc@1: 55.4688 (55.4688)  Acc@5: 95.7031 (95.7031)
2024-04-27 22:03:52,460 - train - INFO - Test: [  39/39]  Time: 0.028 (0.045)  Loss:  1.3975 (1.3065)  Acc@1: 62.5000 (57.1900)  Acc@5: 93.7500 (94.9800)
2024-04-27 22:03:52,815 - train - INFO - Train: 7 [   0/195 (  0%)]  Loss:  1.931897 (1.9319)  Time: 0.271s,  944.65/s  (0.271s,  944.65/s)  LR: 3.880e-04  Data: 0.144 (0.144)
2024-04-27 22:04:01,173 - train - INFO - Train: 7 [  50/195 ( 26%)]  Loss:  2.081048 (1.9910)  Time: 0.163s, 1566.19/s  (0.169s, 1513.30/s)  LR: 3.880e-04  Data: 0.006 (0.010)
2024-04-27 22:04:08,360 - train - INFO - Train: 7 [ 100/195 ( 52%)]  Loss:  2.028133 (1.9918)  Time: 0.152s, 1680.77/s  (0.157s, 1635.02/s)  LR: 3.880e-04  Data: 0.005 (0.008)
2024-04-27 22:04:16,708 - train - INFO - Train: 7 [ 150/195 ( 77%)]  Loss:  2.104338 (1.9802)  Time: 0.164s, 1561.55/s  (0.160s, 1599.99/s)  LR: 3.880e-04  Data: 0.008 (0.007)
2024-04-27 22:04:24,054 - train - INFO - Train: 7 [ 194/195 (100%)]  Loss:  2.033381 (1.9764)  Time: 0.166s, 1545.49/s  (0.162s, 1584.48/s)  LR: 3.880e-04  Data: 0.000 (0.007)
2024-04-27 22:04:24,235 - train - INFO - Test: [   0/39]  Time: 0.180 (0.180)  Loss:  1.2236 (1.2236)  Acc@1: 62.1094 (62.1094)  Acc@5: 96.4844 (96.4844)
2024-04-27 22:04:25,791 - train - INFO - Test: [  39/39]  Time: 0.036 (0.043)  Loss:  1.2275 (1.2504)  Acc@1: 68.7500 (61.0100)  Acc@5: 100.0000 (95.4000)
2024-04-27 22:04:26,160 - train - INFO - Train: 8 [   0/195 (  0%)]  Loss:  1.877050 (1.8771)  Time: 0.289s,  885.69/s  (0.289s,  885.69/s)  LR: 4.420e-04  Data: 0.154 (0.154)
2024-04-27 22:04:34,488 - train - INFO - Train: 8 [  50/195 ( 26%)]  Loss:  1.780174 (1.9381)  Time: 0.166s, 1545.13/s  (0.169s, 1515.32/s)  LR: 4.420e-04  Data: 0.005 (0.009)
2024-04-27 22:04:41,942 - train - INFO - Train: 8 [ 100/195 ( 52%)]  Loss:  1.805658 (1.9415)  Time: 0.117s, 2186.74/s  (0.159s, 1609.00/s)  LR: 4.420e-04  Data: 0.005 (0.007)
