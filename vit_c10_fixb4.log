2024-03-31 15:29:25,413 - train - INFO - Training with a single process on 1 GPUs.
2024-03-31 15:29:40,225 - train - INFO - Model vit_7_4_32 created, param count:3717016
2024-03-31 15:29:40,243 - train - INFO - Using native Torch AMP. Training in mixed precision.
2024-03-31 15:29:40,243 - train - INFO - Scheduled epochs: 310
2024-03-31 15:29:41,818 - train - INFO - Verifying teacher model
2024-03-31 15:29:42,540 - train - INFO - Test: [   0/78]  Time: 0.721 (0.721)  Loss:  0.3445 (0.3445)  Acc@1: 92.9688 (92.9688)  Acc@5: 100.0000 (100.0000)
2024-03-31 15:29:43,151 - train - INFO - Test: [  50/78]  Time: 0.017 (0.026)  Loss:  0.2869 (0.3467)  Acc@1: 96.0938 (93.3211)  Acc@5: 100.0000 (99.6630)
2024-03-31 15:29:43,609 - train - INFO - Test: [  78/78]  Time: 0.099 (0.023)  Loss:  0.3740 (0.3449)  Acc@1: 93.7500 (93.5400)  Acc@5: 100.0000 (99.7600)
2024-03-31 15:29:43,610 - train - INFO - Verifying initial model
2024-03-31 15:29:45,116 - train - INFO - Test: [   0/78]  Time: 1.505 (1.505)  Loss:  2.2656 (2.2656)  Acc@1: 19.5312 (19.5312)  Acc@5: 61.7188 (61.7188)
2024-03-31 15:31:19,464 - train - INFO - Test: [  50/78]  Time: 2.187 (1.879)  Loss:  2.2852 (2.2930)  Acc@1: 12.5000 (14.4455)  Acc@5: 60.9375 (54.2279)
2024-03-31 15:32:14,051 - train - INFO - Test: [  78/78]  Time: 1.747 (1.904)  Loss:  2.2773 (2.2928)  Acc@1: 18.7500 (14.3900)  Acc@5: 56.2500 (54.3200)
2024-03-31 15:32:17,002 - train - INFO - Train: 0 [   0/390 (  0%)]  Loss:  2.302428 (2.3024)  Time: 2.946s,   43.44/s  (2.946s,   43.44/s)  LR: 1.000e-05  Data: 0.710 (0.710)
2024-03-31 15:33:57,570 - train - INFO - Train: 0 [  50/390 ( 13%)]  Loss:  2.300118 (2.2987)  Time: 1.878s,   68.16/s  (2.030s,   63.06/s)  LR: 1.000e-05  Data: 0.003 (0.020)
2024-03-31 15:35:35,651 - train - INFO - Train: 0 [ 100/390 ( 26%)]  Loss:  2.278270 (2.2929)  Time: 1.926s,   66.46/s  (1.996s,   64.13/s)  LR: 1.000e-05  Data: 0.003 (0.014)
2024-03-31 15:37:14,726 - train - INFO - Train: 0 [ 150/390 ( 39%)]  Loss:  2.280651 (2.2892)  Time: 1.869s,   68.49/s  (1.991s,   64.28/s)  LR: 1.000e-05  Data: 0.005 (0.012)
2024-03-31 15:38:54,623 - train - INFO - Train: 0 [ 200/390 ( 51%)]  Loss:  2.289437 (2.2855)  Time: 1.810s,   70.73/s  (1.993s,   64.23/s)  LR: 1.000e-05  Data: 0.013 (0.011)
2024-03-31 15:40:34,165 - train - INFO - Train: 0 [ 250/390 ( 64%)]  Loss:  2.258520 (2.2823)  Time: 1.893s,   67.62/s  (1.992s,   64.24/s)  LR: 1.000e-05  Data: 0.012 (0.010)
2024-03-31 15:42:15,545 - train - INFO - Train: 0 [ 300/390 ( 77%)]  Loss:  2.274234 (2.2794)  Time: 2.220s,   57.65/s  (1.998s,   64.06/s)  LR: 1.000e-05  Data: 0.018 (0.010)
2024-03-31 15:43:56,140 - train - INFO - Train: 0 [ 350/390 ( 90%)]  Loss:  2.246977 (2.2765)  Time: 2.404s,   53.24/s  (2.000s,   63.99/s)  LR: 1.000e-05  Data: 0.013 (0.010)
2024-03-31 15:45:13,149 - train - INFO - Train: 0 [ 389/390 (100%)]  Loss:  2.228477 (2.2746)  Time: 1.859s,   68.85/s  (1.998s,   64.08/s)  LR: 1.000e-05  Data: 0.000 (0.010)
2024-03-31 15:45:14,917 - train - INFO - Test: [   0/78]  Time: 1.762 (1.762)  Loss:  2.1133 (2.1133)  Acc@1: 31.2500 (31.2500)  Acc@5: 79.6875 (79.6875)
