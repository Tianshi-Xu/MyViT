dataset: torch/cifar10
num_classes: 10
img_size: 32
mean:
    - 0.4914
    - 0.4822
    - 0.4465
std:
    - 0.2470
    - 0.2435
    - 0.2616
crop_pct: 1.0
scale:
    - 0.8
    - 1.0
interpolation: bicubic
train_interpolation: random
aa: rand-m9-mstd0.5-inc1
mixup: 0.8
mixup_off_epoch: 175
mixup_prob: 1.0
mixup_mode: batch
mixup_switch_prob: 0.5
cutmix: 1.0
reprob: 0.25
remode: pixel
amp: True
batch_size: 128
lr: 55e-5
min_lr: 1e-5
sched: cosine
weight_decay: 6e-2
epochs: 300
cooldown_epochs: 10
warmup_epochs: 10
warmup_lr: 0.00001
opt: adamw
smoothing: 0.1
workers: 4
use_kd: true
kd_alpha: 4
seed: 3407
checkpoint_hist: 5
# fix_blocksize=-1 represents that the blocksize depends on NAS
# finetune should be false when nas
# lasso_alpha 1e-5 for CIFAR /1e-4 for TinyImageNet
# tau 0.99 is good
finetune: False
lasso_alpha: 0
# b8 delta_w
# fix_blocksize_list: 16,16,2,2,16,16,2,8,16,8,2,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,4,1
# b8 2/4/8/16, delta_z
# fix_blocksize_list: 16,16,2,2,16,8,2,16,16,16,2,16,16,16,8,16,16,16,4,16,16,16,16,16,16,16,16,16,1
# + 16<50% b8
# fix_blocksize_list: 16,16,2,4,16,8,2,16,16,8,2,16,16,8,8,16,16,8,8,16,16,8,8,16,16,8,8,16,1
# + delta_z^2 b8
# fix_blocksize_list: 16,16,2,16,16,8,2,16,16,16,2,16,16,16,2,16,16,16,4,16,16,16,8,16,16,16,16,16,1
# b4
# fix_blocksize_list: 16,16,2,2,16,2,2,16,2,2,2,16,4,2,2,16,16,16,2,16,16,16,2,16,16,16,2,2,1
# b2
# fix_blocksize_list: 8,8,1,1,8,1,1,8,1,1,1,8,4,1,1,8,8,8,1,8,8,8,1,8,8,1,1,1,1
# 0.15 1/2/4/8/16, delta_z,第一个是代码错误的结果
# fix_blocksize_list: 16,16,1,16,16,16,2,16,16,16,8,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,1
# fix_blocksize_list: 16,16,1,8,16,16,1,16,16,16,4,16,16,16,16,16,16,16,8,16,16,16,16,16,16,16,16,16,1
# 1/(1-error rate)
# fix_blocksize_list: 16,4,4,1,16,8,1,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,1
# delta_z, 1/2/4/8/16, 16<50%
# fix_blocksize_list: 16,16,1,8,16,8,2,16,16,8,8,16,16,8,8,16,16,8,8,16,16,16,8,16,16,8,8,8,1
# hawqv3 b8
# fix_blocksize_list: 16,16,8,1,16,8,1,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,1
# b4
# fix_blocksize_list: 16,16,1,1,16,8,1,1,16,8,1,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,1,1
# b8 delta_w_fim test
# fix_blocksize_list: 16,16,2,2,16,8,4,4,16,8,4,16,16,8,8,16,16,16,8,16,16,16,16,16,16,16,4,8,1
# b8 delta_w_fim train
# fix_blocksize_list: 16,16,2,2,16,8,2,8,16,8,4,16,16,16,4,16,16,16,4,16,16,16,16,16,16,16,16,16,1
# b4, 同上
fix_blocksize_list: 16,8,1,1,16,4,2,2,8,2,1,4,8,2,2,8,8,16,4,16,16,16,4,16,16,16,16,16,2
tau: 1
budget: 0.20
fix_blocksize: 1
log_name: vit_c10_b4_fim
teacher: vit_7_4_32
teacher_checkpoint: /home/xts/code/njeans/MyViT/pretrained/vit_c10.pth.tar
# resume: "/home/xts/code/njeans/MySparsity/Ternary-ViT/output/train/20240130-204808-cifar_cir_nas_mobilenetv2-32/model_best.pth.tar"
initial_checkpoint: /home/xts/code/njeans/MyViT/pretrained/vit_c10.pth.tar
# b8
# resume: "/home/xts/code/njeans/Compact-Transformers/output/train/20240331-152914-vit_7_4_32-32/model_best.pth.tar"
# b4
# resume: "/home/xts/code/njeans/Compact-Transformers/output/train/20240331-153214-vit_7_4_32-32/model_best.pth.tar"