2024-04-27 22:02:45,312 - train - INFO - aa: rand-m9-mstd0.5-inc1
amp: true
apex_amp: false
aug_splits: 0
batch_size: 256
bn_eps: null
bn_momentum: null
bn_tf: false
channels_last: false
checkpoint_hist: 3
clip_grad: null
clip_mode: norm
color_jitter: 0.4
cooldown_epochs: 10
crop_pct: 1.0
cutmix: 1.0
cutmix_minmax: null
data_dir: /home/xts/code/dataset/cifar10/
dataset: torch/cifar10
decay_epochs: 30
decay_rate: 0.1
dist_bn: ''
drop: 0.0
drop_block: null
drop_connect: null
drop_path: null
epoch_repeats: 0.0
epochs: 300
eval_metric: top1
experiment: ''
gp: null
hflip: 0.5
img_size: 32
initial_checkpoint: /home/xts/code/njeans/MyViT/pretrained/mbv2_c10.pth.tar
input_size: null
interpolation: bicubic
jsd: false
kd_alpha: 4
local_rank: 0
log_interval: 50
log_name: mbv2_c10_prune_b4
log_wandb: false
lr: 0.00055
lr_cycle_limit: 1
lr_cycle_mul: 1.0
lr_noise: null
lr_noise_pct: 0.67
lr_noise_std: 1.0
mean:
- 0.4914
- 0.4822
- 0.4465
min_lr: 1.0e-05
mixup: 0.8
mixup_mode: batch
mixup_off_epoch: 175
mixup_prob: 1.0
mixup_switch_prob: 0.5
model: c10_prune_mobilenetv2
model_ema: false
model_ema_decay: 0.9998
model_ema_force_cpu: false
momentum: 0.9
native_amp: false
no_aug: false
no_prefetcher: false
no_resume_opt: false
num_classes: 10
opt: adamw
opt_betas: null
opt_eps: null
output: ''
patience_epochs: 10
pin_mem: false
pretrained: false
prune_ratio: 75
ratio:
- 0.75
- 1.3333333333333333
recount: 1
recovery_interval: 0
remode: pixel
reprob: 0.25
resplit: false
resume: ''
save_images: false
scale:
- 0.8
- 1.0
sched: cosine
seed: 3407
smoothing: 0.1
split_bn: false
start_epoch: null
std:
- 0.247
- 0.2435
- 0.2616
sync_bn: false
teacher: c10_mobilenetv2
teacher_checkpoint: /home/xts/code/njeans/MyViT/pretrained/mbv2_c10.pth.tar
torchscript: false
train_interpolation: random
train_split: train
tta: 0
use_kd: true
use_multi_epochs_loader: false
val_split: validation
validation_batch_size_multiplier: 1
vflip: 0.0
warmup_epochs: 10
warmup_lr: 1.0e-05
weight_decay: 0.06
workers: 4

2024-04-27 22:02:45,312 - train - INFO - Training with a single process on 1 GPUs.
2024-04-27 22:02:47,090 - train - INFO - MobileNetV2(
  (features): Sequential(
    (0): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (1): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (7): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (8): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (9): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (12): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (13): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual(
      (conv): Sequential(
        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (classifier): Linear(in_features=1280, out_features=10, bias=True)
)
2024-04-27 22:02:47,708 - train - INFO - Model c10_prune_mobilenetv2 created, param count:2236682
2024-04-27 22:02:47,733 - train - INFO - Using native Torch AMP. Training in mixed precision.
2024-04-27 22:02:47,733 - train - INFO - Scheduled epochs: 310
2024-04-27 22:02:49,162 - train - INFO - Verifying teacher model
2024-04-27 22:02:51,439 - train - INFO - Test: [   0/39]  Time: 2.276 (2.276)  Loss:  0.3931 (0.3931)  Acc@1: 94.1406 (94.1406)  Acc@5: 100.0000 (100.0000)
2024-04-27 22:02:52,669 - train - INFO - Test: [  39/39]  Time: 0.641 (0.088)  Loss:  0.4517 (0.3957)  Acc@1: 93.7500 (94.7100)  Acc@5: 100.0000 (99.8500)
2024-04-27 22:02:52,670 - train - INFO - Verifying initial model
2024-04-27 22:02:52,800 - train - INFO - Test: [   0/39]  Time: 0.128 (0.128)  Loss:  0.3931 (0.3931)  Acc@1: 94.1406 (94.1406)  Acc@5: 100.0000 (100.0000)
2024-04-27 22:02:53,478 - train - INFO - Test: [  39/39]  Time: 0.010 (0.020)  Loss:  0.4517 (0.3957)  Acc@1: 93.7500 (94.7100)  Acc@5: 100.0000 (99.8500)
2024-04-27 22:02:58,478 - train - INFO - Total Mul: 3953.333333333333, Total Rot: 1086.7
2024-04-27 22:03:00,921 - train - INFO - Train: 0 [   0/195 (  0%)]  Loss:  2.491466 (2.4915)  Time: 2.438s,  104.99/s  (2.438s,  104.99/s)  LR: 1.000e-05  Data: 0.329 (0.329)
2024-04-27 22:03:05,900 - train - INFO - Train: 0 [  50/195 ( 26%)]  Loss:  2.500626 (2.5029)  Time: 0.105s, 2442.29/s  (0.145s, 1760.53/s)  LR: 1.000e-05  Data: 0.011 (0.014)
2024-04-27 22:03:10,687 - train - INFO - Train: 0 [ 100/195 ( 52%)]  Loss:  2.400923 (2.4652)  Time: 0.107s, 2401.33/s  (0.121s, 2119.14/s)  LR: 1.000e-05  Data: 0.007 (0.011)
2024-04-27 22:03:15,363 - train - INFO - Train: 0 [ 150/195 ( 77%)]  Loss:  2.322791 (2.4398)  Time: 0.092s, 2793.00/s  (0.112s, 2290.63/s)  LR: 1.000e-05  Data: 0.007 (0.010)
2024-04-27 22:03:19,677 - train - INFO - Train: 0 [ 194/195 (100%)]  Loss:  2.307756 (2.4195)  Time: 0.084s, 3054.90/s  (0.109s, 2356.01/s)  LR: 1.000e-05  Data: 0.000 (0.010)
2024-04-27 22:03:19,780 - train - INFO - Test: [   0/39]  Time: 0.101 (0.101)  Loss:  2.1387 (2.1387)  Acc@1: 22.2656 (22.2656)  Acc@5: 73.4375 (73.4375)
2024-04-27 22:03:20,484 - train - INFO - Test: [  39/39]  Time: 0.014 (0.020)  Loss:  2.0020 (2.1619)  Acc@1: 25.0000 (19.6100)  Acc@5: 87.5000 (70.5400)
2024-04-27 22:03:20,882 - train - INFO - Train: 1 [   0/195 (  0%)]  Loss:  2.325186 (2.3252)  Time: 0.303s,  843.68/s  (0.303s,  843.68/s)  LR: 6.400e-05  Data: 0.209 (0.209)
2024-04-27 22:03:25,551 - train - INFO - Train: 1 [  50/195 ( 26%)]  Loss:  2.265872 (2.2902)  Time: 0.095s, 2682.91/s  (0.097s, 2626.58/s)  LR: 6.400e-05  Data: 0.007 (0.011)
2024-04-27 22:03:30,347 - train - INFO - Train: 1 [ 100/195 ( 52%)]  Loss:  2.213262 (2.2594)  Time: 0.114s, 2238.48/s  (0.097s, 2647.50/s)  LR: 6.400e-05  Data: 0.007 (0.009)
2024-04-27 22:03:35,966 - train - INFO - Train: 1 [ 150/195 ( 77%)]  Loss:  2.136448 (2.2397)  Time: 0.116s, 2216.06/s  (0.102s, 2512.82/s)  LR: 6.400e-05  Data: 0.010 (0.008)
2024-04-27 22:03:40,434 - train - INFO - Train: 1 [ 194/195 (100%)]  Loss:  2.240658 (2.2266)  Time: 0.097s, 2634.87/s  (0.102s, 2514.78/s)  LR: 6.400e-05  Data: 0.000 (0.008)
2024-04-27 22:03:40,575 - train - INFO - Test: [   0/39]  Time: 0.138 (0.138)  Loss:  1.7842 (1.7842)  Acc@1: 35.1562 (35.1562)  Acc@5: 87.5000 (87.5000)
2024-04-27 22:03:41,362 - train - INFO - Test: [  39/39]  Time: 0.019 (0.023)  Loss:  1.7666 (1.8003)  Acc@1: 31.2500 (36.6600)  Acc@5: 87.5000 (88.0700)
2024-04-27 22:03:41,737 - train - INFO - Train: 2 [   0/195 (  0%)]  Loss:  2.153342 (2.1533)  Time: 0.274s,  933.36/s  (0.274s,  933.36/s)  LR: 1.180e-04  Data: 0.171 (0.171)
2024-04-27 22:03:46,863 - train - INFO - Train: 2 [  50/195 ( 26%)]  Loss:  2.172625 (2.1697)  Time: 0.104s, 2464.77/s  (0.106s, 2417.94/s)  LR: 1.180e-04  Data: 0.007 (0.010)
2024-04-27 22:03:51,957 - train - INFO - Train: 2 [ 100/195 ( 52%)]  Loss:  2.204002 (2.1529)  Time: 0.097s, 2631.73/s  (0.104s, 2464.33/s)  LR: 1.180e-04  Data: 0.004 (0.009)
2024-04-27 22:03:57,095 - train - INFO - Train: 2 [ 150/195 ( 77%)]  Loss:  2.168363 (2.1394)  Time: 0.108s, 2365.20/s  (0.103s, 2473.47/s)  LR: 1.180e-04  Data: 0.007 (0.008)
2024-04-27 22:04:03,108 - train - INFO - Train: 2 [ 194/195 (100%)]  Loss:  2.177287 (2.1327)  Time: 0.122s, 2101.65/s  (0.111s, 2306.75/s)  LR: 1.180e-04  Data: 0.000 (0.008)
2024-04-27 22:04:03,244 - train - INFO - Test: [   0/39]  Time: 0.135 (0.135)  Loss:  1.5957 (1.5957)  Acc@1: 44.5312 (44.5312)  Acc@5: 90.6250 (90.6250)
2024-04-27 22:04:04,469 - train - INFO - Test: [  39/39]  Time: 0.013 (0.034)  Loss:  1.6729 (1.5817)  Acc@1: 37.5000 (46.6700)  Acc@5: 93.7500 (92.3800)
2024-04-27 22:04:04,865 - train - INFO - Train: 3 [   0/195 (  0%)]  Loss:  2.146099 (2.1461)  Time: 0.315s,  813.05/s  (0.315s,  813.05/s)  LR: 1.720e-04  Data: 0.188 (0.188)
2024-04-27 22:04:11,373 - train - INFO - Train: 3 [  50/195 ( 26%)]  Loss:  2.044616 (2.1115)  Time: 0.128s, 2000.79/s  (0.134s, 1914.11/s)  LR: 1.720e-04  Data: 0.007 (0.010)
2024-04-27 22:04:17,990 - train - INFO - Train: 3 [ 100/195 ( 52%)]  Loss:  2.004491 (2.0827)  Time: 0.128s, 2003.53/s  (0.133s, 1924.18/s)  LR: 1.720e-04  Data: 0.007 (0.008)
2024-04-27 22:04:24,501 - train - INFO - Train: 3 [ 150/195 ( 77%)]  Loss:  2.144252 (2.0777)  Time: 0.131s, 1951.52/s  (0.132s, 1937.97/s)  LR: 1.720e-04  Data: 0.007 (0.007)
2024-04-27 22:04:30,408 - train - INFO - Train: 3 [ 194/195 (100%)]  Loss:  2.137452 (2.0705)  Time: 0.125s, 2046.17/s  (0.133s, 1930.90/s)  LR: 1.720e-04  Data: 0.000 (0.007)
2024-04-27 22:04:30,526 - train - INFO - Test: [   0/39]  Time: 0.116 (0.116)  Loss:  1.4531 (1.4531)  Acc@1: 51.9531 (51.9531)  Acc@5: 92.9688 (92.9688)
2024-04-27 22:04:31,855 - train - INFO - Test: [  39/39]  Time: 0.015 (0.036)  Loss:  1.5713 (1.4466)  Acc@1: 43.7500 (51.7200)  Acc@5: 93.7500 (93.5800)
2024-04-27 22:04:32,259 - train - INFO - Train: 4 [   0/195 (  0%)]  Loss:  2.046046 (2.0460)  Time: 0.321s,  796.88/s  (0.321s,  796.88/s)  LR: 2.260e-04  Data: 0.187 (0.187)
2024-04-27 22:04:39,070 - train - INFO - Train: 4 [  50/195 ( 26%)]  Loss:  1.915184 (2.0205)  Time: 0.156s, 1643.52/s  (0.140s, 1830.83/s)  LR: 2.260e-04  Data: 0.007 (0.011)
2024-04-27 22:04:45,999 - train - INFO - Train: 4 [ 100/195 ( 52%)]  Loss:  2.023740 (2.0056)  Time: 0.135s, 1893.22/s  (0.139s, 1839.10/s)  LR: 2.260e-04  Data: 0.006 (0.009)
